{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4113ec3d391c157",
   "metadata": {},
   "source": [
    "# Making Expressions Regular: How to Do Regular Expressions in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b766f74d17787e30",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a5cbe80dca7270",
   "metadata": {},
   "source": [
    "There are a multitude of situations in which we deal with text when we program. We could be working on gathering text from messy, scraped webpages. We could be filtering data from a corpus of literature to perform statistical analyses. We could even be determining whether data submitted to us from a public interface is valid and secure. \n",
    "\n",
    "If you've worked in any of these environments, you might've heard of regular expressions. Regular expressions are a highly useful tool in situations like these. They serve as a flexible tool for pattern recognition. When paired with a programming library, such as Python's native `re` module, regular expressions can be weaponized for gathering, filtering, or cleaning text.\n",
    "\n",
    "At the same time, regular expressions can seem like an intimidating formalism. On the one hand, they're based on theoretical principles that we use to analyze language; this, at times, can seem cryptic and abstract. On the other hand, their notation in practice varies depending on the programming language, and the variety of syntactic sugar used to make writing them easier can be a lot to comprehend and learn.\n",
    "\n",
    "Whether you've been facing these struggles, you've just heard of regular expressions and are curious to learn more, or you've wanted to review them in detail, this notebook is for you! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80263ca368dc857b",
   "metadata": {},
   "source": [
    "As a guide to regular expressions, this notebook is structured as follows:\n",
    "- First, we'll talk about the *theory* behind regular expressions. In this section, we'll go over the original concept of regular expressions from formal language theory. We'll see the three basic operations that regular expressions have: alternation, concatenation, and quantification. We'll also use graphs to visualize how regular expressions process text. \n",
    "\t- If you're in a hurry or aren't as interested in background material, you can likely get away with skipping this section. Its material certainly helps to reinforce ideas about regular expressions, but we won't be going over any actual Python notation or code here.\n",
    "- Next, we'll discuss the *practice* of regular expressions. In this section, we'll describe Python's notation for regular expressions, piece-by-piece. You'll learn not only about the basic operations of Python's regular expressions but also the \"syntactic sugar\" that makes writing regular expressions much easier. We'll also touch on the many kinds of *grouping*, as well as other miscellaneous capabilities, that Python supports. \n",
    "- Finally, we'll dive into the API for Python's native `re` library and show what each function and class is used for.\n",
    "With all that said, let's get to making expressions regular!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddad63f0ff40218",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Regular Expressions: In Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7674914744bb8c",
   "metadata": {},
   "source": [
    "Before we talk about how to use regular expressions, let's start by understanding their origins and critical components. \n",
    "\n",
    "Regular expressions are the product of a field known as formal language theory. This field attempts to understand the properties of various languages. In this context, a *language* is a set of permitted sequences of symbols drawn from some alphabet. \n",
    "\n",
    "Consider the following example. Every positive number is composed of the alphabet `{\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"}`. The formal regular expression for it might look like this: `(1 ∪ 2 ∪ 3 ∪ 4 ∪ 5 ∪ 6 ∪ 7 ∪ 8 ∪ 9)(0 ∪ 1 ∪ 2 ∪ 3 ∪ 4 ∪ 5 ∪ 6 ∪ 7 ∪ 8 ∪ 9)*`. Essentially, the expression says \"Choose any digit from 1 to 9. Then, choose zero or more digits from 0 to 9.\" This expression *generates* certain patterns, such as `\"4200\"` or `\"9876543210\"`; at the same time, it *does not generate* certain patterns, such as `\"007\"` (as `\"0\"` is not an option for our first digit) or `\"\"` (as a number must have at least one digit).\n",
    "\n",
    "If our digit example isn't clear, don't worry: we'll go over it in detail in the subsections below. For now, to sum up this discussion, regular expressions are a formalism that describes a language. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a1cc4ba699bc04",
   "metadata": {},
   "source": [
    "Next, we'll be talking about the three operations that make up regular expressions: *alternation*, *concatenation*, and *quantification*. While these terms might seem difficult to parse, they are really just formal ways of expressing \"or\", \"and\", and \"how many?\"\n",
    "\n",
    "To discuss this, we'll provide some visualizations of what regular expressions look like using their companion formalism: a *finite automaton* (pl. *finite automata*). \n",
    "\n",
    "The details of what this formalism entails aren't important for our purposes. What you need to know is that finite automata can equal regular expressions exactly, and they do the same thing that regular expressions do: describe languages. However, finite automata use graphs instead of strings, and they are described in terms of *accepting* or *rejecting* the strings that they process. Each state of a language is a node, and each edge represents a transition between those states. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81e03a1eb5820548",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T18:51:20.775128Z",
     "start_time": "2024-09-10T18:51:20.225333Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"144pt\" height=\"37pt\" viewBox=\"0.00 0.00 144.00 36.50\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 32.5)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-32.5 140,-32.5 140,4 -4,4\"/>\n",
       "<!-- _START -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "\n",
       "</g>\n",
       "<!-- 0 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.33,-21C53.33,-21 43.67,-21 43.67,-21 40.83,-21 38,-18.17 38,-15.33 38,-15.33 38,-9.67 38,-9.67 38,-6.83 40.83,-4 43.67,-4 43.67,-4 53.33,-4 53.33,-4 56.17,-4 59,-6.83 59,-9.67 59,-9.67 59,-15.33 59,-15.33 59,-18.17 56.17,-21 53.33,-21\"/>\n",
       "<text text-anchor=\"start\" x=\"42\" y=\"-10\" font-family=\"Monospace\" font-size=\"10.00\">q0</text>\n",
       "</g>\n",
       "<!-- _START&#45;&gt;0 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1.13,-12.5C2.79,-12.5 19.6,-12.5 32.5,-12.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"37.74,-12.5 32.74,-14.75 35.24,-12.5 32.74,-12.5 32.74,-12.5 32.74,-12.5 35.24,-12.5 32.74,-10.25 37.74,-12.5 37.74,-12.5\"/>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M126.33,-21C126.33,-21 116.67,-21 116.67,-21 113.83,-21 111,-18.17 111,-15.33 111,-15.33 111,-9.67 111,-9.67 111,-6.83 113.83,-4 116.67,-4 116.67,-4 126.33,-4 126.33,-4 129.17,-4 132,-6.83 132,-9.67 132,-9.67 132,-15.33 132,-15.33 132,-18.17 129.17,-21 126.33,-21\"/>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M127.67,-25C127.67,-25 115.33,-25 115.33,-25 111.17,-25 107,-20.83 107,-16.67 107,-16.67 107,-8.33 107,-8.33 107,-4.17 111.17,0 115.33,0 115.33,0 127.67,0 127.67,0 131.83,0 136,-4.17 136,-8.33 136,-8.33 136,-16.67 136,-16.67 136,-20.83 131.83,-25 127.67,-25\"/>\n",
       "<text text-anchor=\"start\" x=\"115\" y=\"-10\" font-family=\"Monospace\" font-size=\"10.00\">q1</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M59.08,-12.5C69.97,-12.5 88.02,-12.5 101.75,-12.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"106.96,-12.5 101.96,-14.75 104.46,-12.5 101.96,-12.5 101.96,-12.5 101.96,-12.5 104.46,-12.5 101.96,-10.25 106.96,-12.5 106.96,-12.5\"/>\n",
       "<text text-anchor=\"start\" x=\"80\" y=\"-18.3\" font-family=\"Monospace\" font-size=\"9.00\">1</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tock import FiniteAutomaton\n",
    "from tock.graphs import Graph, to_graph\n",
    "\n",
    "basic_dfa: FiniteAutomaton = FiniteAutomaton()\n",
    "basic_dfa.set_start_state(\"q0\")\n",
    "basic_dfa.add_accept_state(\"q1\")\n",
    "basic_dfa.add_transitions([\"q0, 1 -> q1\"])\n",
    "\n",
    "graph: Graph = to_graph(basic_dfa)\n",
    "display(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a51f0f94e2596b",
   "metadata": {},
   "source": [
    "The above finite automaton goes between two states: a start state (here, `\"q0\"`) and an accept state (here, `\"q1\"`, which is encircled twice). However, it only does so if the string it's processing begins with a `\"1\"`. In fact, this automaton represents the language `1`: only the string `\"1\"` is in that language. If `\"1\"` doesn't start the string or if it has other symbols following it, the string will be rejected by the automaton.\n",
    "\n",
    "Hopefully, that has given you as sense of how to read an automaton. Now, we'll turn to those three operations that give regular expressions and finite automata their expressive power. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e5d287b457e811",
   "metadata": {},
   "source": [
    "### Alternation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d020161f792cc9e",
   "metadata": {},
   "source": [
    "In general, we like having options: options give us the ability to choose the outcome that's optimal for us. Our languages present options, too, in having many different ways to convey the same idea. Regular expressions come equipped with a mechanism to handle this phenomenon: (exclusive) alternation.\n",
    "\n",
    "Alternation represents the logical idea of \"exclusive or\": one or another option can be chosen---but not both. In a regular expression, we use the symbol `∪` to provide two (or more) symbols as alternatives. For instance, the regular expression `0 ∪ 1` indicates the language containing strings `\"0\"` or `\"1\"`. We can model this via the finite automaton below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12bf5aec1f3cd987",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T18:51:21.247595Z",
     "start_time": "2024-09-10T18:51:20.778083Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"144pt\" height=\"76pt\" viewBox=\"0.00 0.00 144.00 76.00\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 72)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-72 140,-72 140,4 -4,4\"/>\n",
       "<!-- _START -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.33,-42C53.33,-42 43.67,-42 43.67,-42 40.83,-42 38,-39.17 38,-36.33 38,-36.33 38,-30.67 38,-30.67 38,-27.83 40.83,-25 43.67,-25 43.67,-25 53.33,-25 53.33,-25 56.17,-25 59,-27.83 59,-30.67 59,-30.67 59,-36.33 59,-36.33 59,-39.17 56.17,-42 53.33,-42\"/>\n",
       "<text text-anchor=\"start\" x=\"42\" y=\"-31\" font-family=\"Monospace\" font-size=\"10.00\">q0</text>\n",
       "</g>\n",
       "<!-- _START&#45;&gt;2 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1.13,-33.5C2.79,-33.5 19.6,-33.5 32.5,-33.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"37.74,-33.5 32.74,-35.75 35.24,-33.5 32.74,-33.5 32.74,-33.5 32.74,-33.5 35.24,-33.5 32.74,-31.25 37.74,-33.5 37.74,-33.5\"/>\n",
       "</g>\n",
       "<!-- 0 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M126.33,-64C126.33,-64 116.67,-64 116.67,-64 113.83,-64 111,-61.17 111,-58.33 111,-58.33 111,-52.67 111,-52.67 111,-49.83 113.83,-47 116.67,-47 116.67,-47 126.33,-47 126.33,-47 129.17,-47 132,-49.83 132,-52.67 132,-52.67 132,-58.33 132,-58.33 132,-61.17 129.17,-64 126.33,-64\"/>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M127.67,-68C127.67,-68 115.33,-68 115.33,-68 111.17,-68 107,-63.83 107,-59.67 107,-59.67 107,-51.33 107,-51.33 107,-47.17 111.17,-43 115.33,-43 115.33,-43 127.67,-43 127.67,-43 131.83,-43 136,-47.17 136,-51.33 136,-51.33 136,-59.67 136,-59.67 136,-63.83 131.83,-68 127.67,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"115\" y=\"-53\" font-family=\"Monospace\" font-size=\"10.00\">q1</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M126.33,-21C126.33,-21 116.67,-21 116.67,-21 113.83,-21 111,-18.17 111,-15.33 111,-15.33 111,-9.67 111,-9.67 111,-6.83 113.83,-4 116.67,-4 116.67,-4 126.33,-4 126.33,-4 129.17,-4 132,-6.83 132,-9.67 132,-9.67 132,-15.33 132,-15.33 132,-18.17 129.17,-21 126.33,-21\"/>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M127.67,-25C127.67,-25 115.33,-25 115.33,-25 111.17,-25 107,-20.83 107,-16.67 107,-16.67 107,-8.33 107,-8.33 107,-4.17 111.17,0 115.33,0 115.33,0 127.67,0 127.67,0 131.83,0 136,-4.17 136,-8.33 136,-8.33 136,-16.67 136,-16.67 136,-20.83 131.83,-25 127.67,-25\"/>\n",
       "<text text-anchor=\"start\" x=\"115\" y=\"-10\" font-family=\"Monospace\" font-size=\"10.00\">q2</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;0 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M59.08,-36.47C70.07,-39.87 88.34,-45.54 102.11,-49.8\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"106.96,-51.3 101.51,-51.97 104.57,-50.56 102.18,-49.82 102.18,-49.82 102.18,-49.82 104.57,-50.56 102.85,-47.67 106.96,-51.3 106.96,-51.3\"/>\n",
       "<text text-anchor=\"start\" x=\"80\" y=\"-50.3\" font-family=\"Monospace\" font-size=\"9.00\">0</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;1 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M59.21,-26.52C64.27,-23.24 70.71,-19.59 77,-17.5 84.85,-14.89 93.9,-13.6 101.69,-12.97\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"106.86,-12.64 102.02,-15.2 104.37,-12.8 101.88,-12.96 101.88,-12.96 101.88,-12.96 104.37,-12.8 101.73,-10.71 106.86,-12.64 106.86,-12.64\"/>\n",
       "<text text-anchor=\"start\" x=\"80\" y=\"-23.3\" font-family=\"Monospace\" font-size=\"9.00\">1</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tock import FiniteAutomaton\n",
    "from tock.graphs import Graph, to_graph\n",
    "\n",
    "alternation_dfa: FiniteAutomaton = FiniteAutomaton()\n",
    "alternation_dfa.set_start_state(\"q0\")\n",
    "alternation_dfa.add_accept_state(\"q1\")\n",
    "alternation_dfa.add_accept_state(\"q2\")\n",
    "alternation_dfa.add_transitions([\n",
    "\t\"q0, 0 -> q1\",\n",
    "\t\"q0, 1 -> q2\"\n",
    "])\n",
    "\n",
    "graph: Graph = to_graph(alternation_dfa)\n",
    "display(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ea1e834492283c",
   "metadata": {},
   "source": [
    "As we can see, the graph splits from the start state, `q0`, to two accept states, `q1` and `q2`. Each accept state has a distinct path, conditioned on the presence of either `\"0\"` or `\"1\"` at the beginning of the string, between the start state and itself.\n",
    "\n",
    " Our original positive number regular expression extensively uses alternation. We can see that the first digit is an alternation between all digits but 0, and the other digits are an alternation between every possible digit. We could similarly expand our finite automaton to represent these many alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba04032320757c63",
   "metadata": {},
   "source": [
    "### Concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd6a6140fefdba8",
   "metadata": {},
   "source": [
    "While we sometimes want options to choose from, at other times we want it all: we must pick every option. Languages also feature such requirements in their syntax. For instance, when using the word \"and\" in a complete sentence, we need at least two phrases to join with that word. We don't permit saying something like \"I bought eggs and\" or \"I bought and milk\"; indeed, only \"I bought eggs and milk\" maintains correct syntax.\n",
    "\n",
    "Concatenation expresses the logical concept of \"and\". However, unlike the traditional logical operator of \"and\", as well as the \"and\" we use in everyday speech, the order of \"and\"'s conjuncts matters. In short, to a regular expression, saying \"eggs and milk\" is different from saying \"milk and eggs\", even though these phrases mean the same thing to us.\n",
    "\n",
    "Concatenation is symbolized simply by placing symbols next to one another. For instance, if we wanted to accept the string `\"10\"`, then we would simply write the language `\"10\"`. Its automaton would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa01b216d56318ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T18:51:21.532587Z",
     "start_time": "2024-09-10T18:51:21.249591Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"213pt\" height=\"37pt\" viewBox=\"0.00 0.00 213.00 36.50\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 32.5)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-32.5 209,-32.5 209,4 -4,4\"/>\n",
       "<!-- _START -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "\n",
       "</g>\n",
       "<!-- 0 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.33,-21C53.33,-21 43.67,-21 43.67,-21 40.83,-21 38,-18.17 38,-15.33 38,-15.33 38,-9.67 38,-9.67 38,-6.83 40.83,-4 43.67,-4 43.67,-4 53.33,-4 53.33,-4 56.17,-4 59,-6.83 59,-9.67 59,-9.67 59,-15.33 59,-15.33 59,-18.17 56.17,-21 53.33,-21\"/>\n",
       "<text text-anchor=\"start\" x=\"42\" y=\"-10\" font-family=\"Monospace\" font-size=\"10.00\">q0</text>\n",
       "</g>\n",
       "<!-- _START&#45;&gt;0 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1.13,-12.5C2.79,-12.5 19.6,-12.5 32.5,-12.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"37.74,-12.5 32.74,-14.75 35.24,-12.5 32.74,-12.5 32.74,-12.5 32.74,-12.5 35.24,-12.5 32.74,-10.25 37.74,-12.5 37.74,-12.5\"/>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M122.33,-21C122.33,-21 112.67,-21 112.67,-21 109.83,-21 107,-18.17 107,-15.33 107,-15.33 107,-9.67 107,-9.67 107,-6.83 109.83,-4 112.67,-4 112.67,-4 122.33,-4 122.33,-4 125.17,-4 128,-6.83 128,-9.67 128,-9.67 128,-15.33 128,-15.33 128,-18.17 125.17,-21 122.33,-21\"/>\n",
       "<text text-anchor=\"start\" x=\"111\" y=\"-10\" font-family=\"Monospace\" font-size=\"10.00\">q1</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M59.1,-12.5C70.24,-12.5 88.75,-12.5 101.73,-12.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"106.92,-12.5 101.92,-14.75 104.42,-12.5 101.92,-12.5 101.92,-12.5 101.92,-12.5 104.42,-12.5 101.92,-10.25 106.92,-12.5 106.92,-12.5\"/>\n",
       "<text text-anchor=\"start\" x=\"80\" y=\"-18.3\" font-family=\"Monospace\" font-size=\"9.00\">1</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M195.33,-21C195.33,-21 185.67,-21 185.67,-21 182.83,-21 180,-18.17 180,-15.33 180,-15.33 180,-9.67 180,-9.67 180,-6.83 182.83,-4 185.67,-4 185.67,-4 195.33,-4 195.33,-4 198.17,-4 201,-6.83 201,-9.67 201,-9.67 201,-15.33 201,-15.33 201,-18.17 198.17,-21 195.33,-21\"/>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M196.67,-25C196.67,-25 184.33,-25 184.33,-25 180.17,-25 176,-20.83 176,-16.67 176,-16.67 176,-8.33 176,-8.33 176,-4.17 180.17,0 184.33,0 184.33,0 196.67,0 196.67,0 200.83,0 205,-4.17 205,-8.33 205,-8.33 205,-16.67 205,-16.67 205,-20.83 200.83,-25 196.67,-25\"/>\n",
       "<text text-anchor=\"start\" x=\"184\" y=\"-10\" font-family=\"Monospace\" font-size=\"10.00\">q2</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;1 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M128.08,-12.5C138.97,-12.5 157.02,-12.5 170.75,-12.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"175.96,-12.5 170.96,-14.75 173.46,-12.5 170.96,-12.5 170.96,-12.5 170.96,-12.5 173.46,-12.5 170.96,-10.25 175.96,-12.5 175.96,-12.5\"/>\n",
       "<text text-anchor=\"start\" x=\"149\" y=\"-18.3\" font-family=\"Monospace\" font-size=\"9.00\">0</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tock import FiniteAutomaton\n",
    "from tock.graphs import Graph, to_graph\n",
    "\n",
    "concatenation_dfa: FiniteAutomaton = FiniteAutomaton()\n",
    "concatenation_dfa.set_start_state(\"q0\")\n",
    "concatenation_dfa.add_accept_state(\"q2\")\n",
    "concatenation_dfa.add_transitions([\n",
    "\t\"q0, 1 -> q1\",\n",
    "\t\"q1, 0 -> q2\"\n",
    "])\n",
    "\n",
    "graph: Graph = to_graph(concatenation_dfa)\n",
    "display(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658b2b789feed1b4",
   "metadata": {},
   "source": [
    "As this automaton displays, you need a string beginning with `\"1\"` and followed by `\"0\"` (with nothing else afterward) to reach the accept state, `q2`. Notably, we have a middle state here which is neither a start state nor an accept state; due to concatenation, we accumulate intermediate states that the automaton can be in as it is processing a string.\n",
    "\n",
    "To revisit our positive number regular expression, we apply concatenation to join the first alternation between digits 1 through 9 `(1 ∪ 2 ∪ 3 ∪ 4 ∪ 5 ∪ 6 ∪ 7 ∪ 8 ∪ 9)` with the second alternation containing all digits `(0 ∪ 1 ∪ 2 ∪ 3 ∪ 4 ∪ 5 ∪ 6 ∪ 7 ∪ 8 ∪ 9)*`. As you can see, we can concatenate individual symbols as well as alternations between symbols, permitting a variety of symbolic combinations for our languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b24c8215e2c7ef",
   "metadata": {},
   "source": [
    "### Quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbc04dc26f5a08",
   "metadata": {},
   "source": [
    "Like our other operators, quantification can be considered as a choice. However, instead of choosing between alternatives or conditioning choices on others, quantification indicates how many times a choice can be selected. \n",
    "\n",
    "In natural language, we see variable quantification with items in a series; both with \"and\" and \"or\", we could link as many conjuncts together as we wanted; we are only required to add the conjunction between the last two conjuncts. For instance, the sentences \"I like the colors red and blue\" is correct; so is \"I like the colors red, green, and blue\"; and so is \"I like the colors red, purple, orange, yellow, black, green, white, and blue\".\n",
    "\n",
    "Regular expressions quantify an option by using the Kleene star, `*`, introduced by Stephen Kleene. This operator permits *zero or more* of a symbol or expression to be used. Consider the regular expression `1*`, depicted by the automaton below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2a3f2b54cc0adc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T18:51:21.611577Z",
     "start_time": "2024-09-10T18:51:21.534590Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"75pt\" height=\"67pt\" viewBox=\"0.00 0.00 75.00 67.00\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 63)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-63 71,-63 71,4 -4,4\"/>\n",
       "<!-- _START -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "\n",
       "</g>\n",
       "<!-- 0 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M57.33,-21C57.33,-21 47.67,-21 47.67,-21 44.83,-21 42,-18.17 42,-15.33 42,-15.33 42,-9.67 42,-9.67 42,-6.83 44.83,-4 47.67,-4 47.67,-4 57.33,-4 57.33,-4 60.17,-4 63,-6.83 63,-9.67 63,-9.67 63,-15.33 63,-15.33 63,-18.17 60.17,-21 57.33,-21\"/>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M58.67,-25C58.67,-25 46.33,-25 46.33,-25 42.17,-25 38,-20.83 38,-16.67 38,-16.67 38,-8.33 38,-8.33 38,-4.17 42.17,0 46.33,0 46.33,0 58.67,0 58.67,0 62.83,0 67,-4.17 67,-8.33 67,-8.33 67,-16.67 67,-16.67 67,-20.83 62.83,-25 58.67,-25\"/>\n",
       "<text text-anchor=\"start\" x=\"46\" y=\"-10\" font-family=\"Monospace\" font-size=\"10.00\">q0</text>\n",
       "</g>\n",
       "<!-- _START&#45;&gt;0 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1.14,-12.5C2.84,-12.5 19.1,-12.5 32.69,-12.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"37.92,-12.5 32.92,-14.75 35.42,-12.5 32.92,-12.5 32.92,-12.5 32.92,-12.5 35.42,-12.5 32.92,-10.25 37.92,-12.5 37.92,-12.5\"/>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M47.15,-25.03C45.4,-34.03 47.19,-43 52.5,-43 56.82,-43 58.8,-37.08 58.46,-30.03\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"57.85,-25.03 60.69,-29.72 58.16,-27.51 58.46,-30 58.46,-30 58.46,-30 58.16,-27.51 56.22,-30.27 57.85,-25.03 57.85,-25.03\"/>\n",
       "<text text-anchor=\"start\" x=\"49.5\" y=\"-48.8\" font-family=\"Monospace\" font-size=\"9.00\">1</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tock import FiniteAutomaton\n",
    "from tock.graphs import Graph, to_graph\n",
    "\n",
    "quantification_dfa: FiniteAutomaton = FiniteAutomaton()\n",
    "quantification_dfa.set_start_state(\"q0\")\n",
    "quantification_dfa.add_accept_state(\"q0\")\n",
    "quantification_dfa.add_transitions([\n",
    "\t\"q0, 1 -> q0\",\n",
    "])\n",
    "\n",
    "graph: Graph = to_graph(quantification_dfa)\n",
    "display(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82f41cec3db4461",
   "metadata": {},
   "source": [
    "Strings accepted by this automaton include `\"\"` (the empty string) as well as `\"1\"`, `\"1111\"`, and `\"1111111111\"`, among others. A string of 100 `1`s would also be accepted! Any of these strings would result in our string returning to `q0`: as long as our string contained only `1`s, it would be accepted by this finite automaton.\n",
    "\n",
    "Revisiting our positive number regular expression, the `*` is used in the second alternation. In other words, it allows for the selection of one of the nine digits zero or more times. With the concatenation of the initial digit before it, this results in a number at least one digit long and at most as many digits as desired. Moreover, because we are permitted to choose a different digit with each use of the Kleene star, the pattern permits any sequence of digits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aaf8746b4ab998",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4090118be9b650",
   "metadata": {},
   "source": [
    "At this stage, we've seen all three operations for the original regular expressions: *alternation*, *concatenation*, and *quantification*. We've not only seen these operations independently within regular expression and finite automata, but we've also seen them work together to form any positive integer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5185a2f4e914c67a",
   "metadata": {},
   "source": [
    "Let's do some recap! \n",
    "\n",
    "First, you'll tackle questions on the three operations that we discussed. Please note that one (and exactly one) option will be correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0b030f6d9037192",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T18:51:21.874128Z",
     "start_time": "2024-09-10T18:51:21.613580Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f238d5a941aa422b94b08dc9aadbc5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultipleChoiceQuestion(children=(Output(), RadioButtons(layout=Layout(width='max-content'), options=('ab', 'aa…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.activities.question_table import QUESTION_TABLE\n",
    "\n",
    "display(QUESTION_TABLE[\"theory-alternation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "167f6bc572ccf199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b654fccc834b7da9bd341878100bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultipleChoiceQuestion(children=(Output(), RadioButtons(layout=Layout(width='max-content'), options=('ε', 'abc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.activities.question_table import QUESTION_TABLE\n",
    "\n",
    "display(QUESTION_TABLE[\"theory-concatenation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28777912fc89de0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c1923f45da4ba0b0fb92ebd1412022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultipleChoiceQuestion(children=(Output(), RadioButtons(layout=Layout(width='max-content'), options=('ε', 'a',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.activities.question_table import QUESTION_TABLE\n",
    "\n",
    "display(QUESTION_TABLE[\"theory-quantification\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551bb8d3f5fc08f6",
   "metadata": {},
   "source": [
    "Now that we've gone through the basics, let's reconsider our positive integer regular expression. The questions below will ask you to extend that expression to be more flexible using the three operations that you've learned.\n",
    "\n",
    "Please note that, in the following questions, *at least* one answer will be correct. However, you will only need to select one answer. If you want, feel free to try and find all correct answers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37e34f1c802d57dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b76ad93f3824540bb9cd3acda01d1c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultipleChoiceQuestion(children=(Output(), RadioButtons(layout=Layout(width='max-content'), options=('ε', '(-|…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.activities.question_table import QUESTION_TABLE\n",
    "\n",
    "display(QUESTION_TABLE[\"theory-negative-numbers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10bbb5e1af52001b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0ad2235ee9476ea73f75172fbf8663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultipleChoiceQuestion(children=(Output(), RadioButtons(layout=Layout(width='max-content'), options=('.(0 ∪ 1 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.activities.question_table import QUESTION_TABLE\n",
    "\n",
    "display(QUESTION_TABLE[\"theory-decimals\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af35b32f21901bab",
   "metadata": {},
   "source": [
    "Now that you've had some practice with regular expressions in theory, let's move on to the core of this notebook: doing regular expressions in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5057a3fdc45efa54",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Regular Expressions: In Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980080bbd81c9ed6",
   "metadata": {},
   "source": [
    "The core intention of a regular expression is to supply a concise way to describe a set of strings. For instance, if we wanted to describe the set of valid times on a 24-hour clock, we'd rather write `(?P<hour>[01][0-9]|[2][0-3]):(?P<minute>[0-5][0-9])` than enumerate every possible time (all 1,440 of them). \n",
    "\n",
    "Each programming language defines its own way of writing regular expressions, although many of them employ similar notation and syntax. In this tutorial, we'll focus on Python, whose regular expression syntax is in turn based on Perl. \n",
    "\n",
    "Unlike their original theoretical counterparts, regular expressions in Python include syntactic sugar and additional mechanisms to make working with regular expressions easier. If you read the prior section, you likely noticed that the expression for times on a 24-hour clock is quite dissimilar to the formal notation. Similarly, to capture all positive numbers, we would write `[1-9][0-9]*` or even just `[1-9]\\d*`.\n",
    "\n",
    "Before we begin discussing Python's regular expression syntax, let's imagine a scenario where we might want to use a regular expression. You've probably made an account on a website where you've had to make a password with certain constraints. For example:\n",
    "- ... it must have a certain number of characters. \n",
    "- ... it must include one capital letter and one lowercase letter.\n",
    "- ... it must include at least one number.\n",
    "- ... it must include at least one special character.\n",
    "In our examples below, we'll model some of these constraints and restricted versions of others. These examples should also help to make sense of the clock times and the positive number regular expression given above.\n",
    "\n",
    "One more aside before we begin: the code examples below will use a custom class, `RegexVerifier`, to display the results of applying regular expressions. This class hides away some details of applying regular expressions so that we can first focus on notation; then, in the next section, we'll examine the primitives of the `re` library. Furthermore, after you've read this notebook, feel free to examine the `regex_verifier.py` file for more details on how this class was constructed!\n",
    "\n",
    "With all that said, we'll start off our journey in learning about Python's regular expressions by discussing three major operations they inherit from theoretical regular expressions: *alternation*, *concatenation*, and *quantification*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcb122a21244156",
   "metadata": {},
   "source": [
    "### Alternation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca399b6bdd90e57",
   "metadata": {},
   "source": [
    "Passwords are valuable as a way of protecting an account partially because we have choices. If we had a rather restrictive set of choices for a password, someone could try all options and eventually break into our accounts. (Indeed, even with large sets of options, brute force algorithms still attempt this on some websites!)\n",
    "\n",
    "Python's regular expressions provide options through (exclusive) *alternation*, which is a formal term for the logical \"XOR\" or \"exclusive or\" operator. Python uses two main kinds of notation to express alternation.\n",
    "- The first kind of alternation, referred to here as *bar alteration*, which is the most basic and most flexible, employs the `|` operator.\n",
    "- The second kind of alternation, referred to here as *bracket alternation*, uses square brackets; that is, it surrounds characters with `[` and `]`.\n",
    "\n",
    "To understand how each of these work, let's see them in action. Suppose that we want our passwords to end with a specific digit, such as `\"0\"` or `\"1\"`. Then, we could write any of the following regular expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a052cd518fd258e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular expression \"0|1\" does not match string \"\".\n",
      "Regular expression \"0|1\" matches \"0\".\n",
      "Regular expression \"0|1\" matches \"1\".\n",
      "Regular expression \"0|1\" does not match string \"01\".\n",
      "Regular expression \"0|1\" does not match string \"10\".\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Regular expression \"[01]\" does not match string \"\".\n",
      "Regular expression \"[01]\" matches \"0\".\n",
      "Regular expression \"[01]\" matches \"1\".\n",
      "Regular expression \"[01]\" does not match string \"01\".\n",
      "Regular expression \"[01]\" does not match string \"10\".\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from utils.widgets.regex_verifier import RegexVerifier\n",
    "\n",
    "first_pattern: str = \"0|1\"\n",
    "second_pattern: str = \"[01]\"\n",
    "patterns: list[str] = [first_pattern, second_pattern]\n",
    "strings: list[str] = [\"\", \"0\", \"1\", \"01\", \"10\"]\n",
    "\n",
    "for alternation_pattern in patterns:\n",
    "\tfor alternation_string in strings:\n",
    "\t\tRegexVerifier.apply_regex(alternation_pattern, alternation_string)\n",
    "\telse:\n",
    "\t\tprint(\"-\" * 100)   # Permits break between use of each pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eea27a2b2ac65f3",
   "metadata": {},
   "source": [
    "The above example proposes two patterns--one for each variant of alternation. Each pattern matches *either* the symbol `\"0\"` or the symbol `\"1\"`. Both symbols together, however, will not be matched; similarly, the empty string will not be matched.\n",
    "\n",
    "We could also add further options to both. For example, suppose that we wanted to add the digit `\"2\"`. Then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "feb56357f640191c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular expression \"0|1|2\" matches \"0\".\n",
      "Regular expression \"0|1|2\" matches \"1\".\n",
      "Regular expression \"0|1|2\" matches \"2\".\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Regular expression \"[012]\" matches \"0\".\n",
      "Regular expression \"[012]\" matches \"1\".\n",
      "Regular expression \"[012]\" matches \"2\".\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from utils.widgets.regex_verifier import RegexVerifier\n",
    "\n",
    "first_pattern: str = \"0|1|2\"\n",
    "second_pattern: str = \"[012]\"\n",
    "patterns: list[str] = [first_pattern, second_pattern]\n",
    "strings: list[str] = [\"0\", \"1\", \"2\"]\n",
    "\n",
    "for alternation_pattern in patterns:\n",
    "\tfor alternation_string in strings:\n",
    "\t\tRegexVerifier.apply_regex(alternation_pattern, alternation_string)\n",
    "\telse:\n",
    "\t\tprint(\"-\" * 100)   # Permits break between use of each pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf6236e676d7605",
   "metadata": {},
   "source": [
    "As we can see, the addition of a new symbol can be done easily in both kinds of alternation. For the former, a new `|` needs to be added for each option in alternation; for the latter, the character simply needs to be incorporated into the square brackets.\n",
    "\n",
    "Python provides a convenient shorthand for ordered sets of characters in bracket notation. For instance, digits are ordered from 0 to 9. As a result, instead of writing `[012]`, we could write `[0-2]` (or `[0-9]` for all digits). For alphabetical characters, we could do this as well: `[a-z]` and `[A-Z]` represent the sets of lowercase and uppercase characters, respectively. \n",
    "\n",
    "We'll see later that bar and bracket alternation aren't *exactly* equivalent, but both permit selecting one among different symbol options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc3901cb83ead0",
   "metadata": {},
   "source": [
    "Python can also apply a third sort of alternation. This sort is much less flexible than the other two. It uses special notation or escaped characters as shortcuts to sets of characters that you could manually define with the other two methods. Here are some examples:\n",
    "- \".\": the set of all symbols.\n",
    "- `\"\\d\"`: the set of all numerical digits (*i.e.*, `\"0\"`, `\"1\"`, `\"2\"`, and so on).\n",
    "- `\"\\s\"`: the set of all whitespace characters, here defined as the space (`\" \"`), the newline character (`\"\\n\"`), the carriage return (`\"\\r\"`), the form-feed character (`\"\\f\"`), and the vertical tab character (`\"\\v\"`).\n",
    "- `\"\\w\"`: the set of all *word* characters. The definition of \"word\" depends on whether the text encoding of the regular expression is considered ASCII or Unicode. If it is the former, then all uppercase characters, all lowercase characters, all digits, and the underscore are included; if it is the latter, a larger set of characters are included. For ease of use, you might consider this to be a near-opposite to `\"\\s\"`. Python's string function `.isalnum()` only returns `True` if all characters are word characters, so it can be used to differentiate word and non-word characters.\n",
    "\n",
    "Other important examples of alternation shortcuts are *inverses* of those presented above. In other words, there is also a `\"\\D\"`, `\"\\S\"`, and `\"\\W\"` which represent all characters *not* in `\"\\d\"`, `\"\\s\"`, and `\"\\w\"`, respectively. (Note that the empty string is ***not*** included in the inversion.)\n",
    "\n",
    "We can also invert a custom set of characters for alternation, but we can only do so with the *second* alternation notation. To do this, we use the special character `^` after the left bracket. For instance, we could manually write `\"\\D\"`, the inversion of `\"\\d\"`, like this: `\"[^0123456789]\"`. The code below gives some evidence for their equivalence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96b5243f7b46f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular expression \"\\D\" does not match string \"\".\n",
      "Regular expression \"\\D\" does not match string \"0\".\n",
      "Regular expression \"\\D\" does not match string \"1\".\n",
      "Regular expression \"\\D\" matches \"a\".\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Regular expression \"[^0123456789]\" does not match string \"\".\n",
      "Regular expression \"[^0123456789]\" does not match string \"0\".\n",
      "Regular expression \"[^0123456789]\" does not match string \"1\".\n",
      "Regular expression \"[^0123456789]\" matches \"a\".\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from utils.widgets.regex_verifier import RegexVerifier\n",
    "\n",
    "first_pattern: str = \"\\D\"\n",
    "second_pattern: str = \"[^0123456789]\"\n",
    "patterns: list[str] = [first_pattern, second_pattern]\n",
    "strings: list[str] = [\"\", \"0\", \"1\", \"a\"]\n",
    "\n",
    "for alternation_pattern in patterns:\n",
    "\tfor alternation_string in strings:\n",
    "\t\tRegexVerifier.apply_regex(alternation_pattern, alternation_string)\n",
    "\telse:\n",
    "\t\tprint(\"-\" * 100)   # Permits break between use of each pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eec3f618437dfac",
   "metadata": {},
   "source": [
    "To sum up this subsection, Python provides us with many options for displaying our options. On the one hand, as we'll see in a later subsection (see \"Concatenation\" and \"Grouping\"), bar alternation is a more powerful, if more verbose, notation; on the other hand, bracket notation has a lot of syntactic sugar baked into it that makes it easy to use for selecting from a single set of symbols.\n",
    "\n",
    "Before moving on, feel free to try the exercises below to check your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e346be716bb0d3",
   "metadata": {},
   "source": [
    "#### Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a397723d35b95f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac386c30116d479c94144ad3c765b5ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultipleChoiceQuestion(children=(Output(), RadioButtons(layout=Layout(width='max-content'), options=('~|!|@|#'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.activities.question_table import QUESTION_TABLE\n",
    "\n",
    "display(QUESTION_TABLE[\"password-special-character\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7cea9c30464367a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "090336b7ab0d40129871de0b68b37835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultipleChoiceQuestion(children=(Output(), RadioButtons(layout=Layout(width='max-content'), options=('\\\\d', '[…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.activities.question_table import QUESTION_TABLE\n",
    "\n",
    "display(QUESTION_TABLE[\"password-single-digit\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31fdfedd68006ff",
   "metadata": {},
   "source": [
    "### Concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd7bb094417501d",
   "metadata": {},
   "source": [
    "With alternation, we discussed how options permitted us to provide a stronger password. Of course, having one choice to make will only get us so far; on the other hand, if we join multiple choices together, we vastly expand our options for passwords! \n",
    "\n",
    "Python's regular expressions allow us to order and join multiple items together through *concatenation*, which is equivalent to an ordered version of the logical \"AND\" operation. The manner in which we do this in Python's syntax is quite simple: we simply place each item next to the other one. Of course, since space characters are also valid characters to match, we mean that items should be *directly* adjacent with no intervening characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52452ac310e7e10",
   "metadata": {},
   "source": [
    "Let's see this in action with an example. Suppose that, in our password, we want the following items:\n",
    "1. A capital letter,\n",
    "2. a lowercase letter,\n",
    "3. a numerical digit, and\n",
    "4. a special character.\n",
    "\n",
    "We'll assume that we want these items in the order presented. Let's consider each choice in turn.\n",
    "1. First, we need to pick a capital letter. From the last section, we saw that `[A-Z]` is the set of all capital letters.\n",
    "2. Second, we need to pick a lowercase letter. We similarly saw in the last section that `[a-z]` is the set of all lowercase letters.\n",
    "3. Third, we need to pick a numerical digit. Since there's a shortcut for the set of numerical digits, we'll use that: `\\d`.\n",
    "4. Finally, we want the set of special characters. Unfortunately, there's not a convenient shortcut for all special characters. For our purposes, we'll limit our special characters to the set discussed in the previous section: `\"~\"`, `\"!\"`, and `\"@\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6989bc721d10cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular expression \"[A-Z][a-z]\\d[~!@]\" does not match string \"\".\n",
      "Regular expression \"[A-Z][a-z]\\d[~!@]\" does not match string \"A\".\n",
      "Regular expression \"[A-Z][a-z]\\d[~!@]\" does not match string \"Aa\".\n",
      "Regular expression \"[A-Z][a-z]\\d[~!@]\" does not match string \"Aa0\".\n",
      "Regular expression \"[A-Z][a-z]\\d[~!@]\" matches \"Aa0~\".\n",
      "Regular expression \"[A-Z][a-z]\\d[~!@]\" does not match string \"Aa0~B\".\n",
      "Regular expression \"[A-Z][a-z]\\d[~!@]\" does not match string \"~0aA\".\n"
     ]
    }
   ],
   "source": [
    "from utils.widgets.regex_verifier import RegexVerifier\n",
    "\n",
    "capital_letter_item: str = \"[A-Z]\"\n",
    "lowercase_letter_item: str = \"[a-z]\"\n",
    "numerical_digit_item: str = \"\\d\"\n",
    "special_character_item: str = \"[~!@]\"\n",
    "concatenation_pattern: str = capital_letter_item + lowercase_letter_item + numerical_digit_item + special_character_item\n",
    "\n",
    "strings: list[str] = [\n",
    "\t\"\",\n",
    "\t\"A\",\n",
    "\t\"Aa\",\n",
    "\t\"Aa0\",\n",
    "\t\"Aa0~\",\n",
    "\t\"Aa0~B\",\n",
    "\t\"~0aA\"\n",
    "]\n",
    "\n",
    "for concatenation_string in strings:\n",
    "\tRegexVerifier.apply_regex(concatenation_pattern, concatenation_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc2d4204b78f376",
   "metadata": {},
   "source": [
    "From the results of applying our regular expression to the given set of strings, we can see that only a password with four symbols in the order of capital letter, lowercase letter, numerical digit, and special character derives a match. The empty string (`\"\"`), partial matches (*e.g.*, `\"Aa0\"`), extensions (*e.g.*, `\"Aa0~B\"`), and reorderings (*e.g.*, `\"~0aA\"`) will not work. In this way, concatenation is restrictive: it specifies both the type and position of symbols.\n",
    "\n",
    "You might notice that our example above used bracket alternation and shortcuts in its regular expression; what about bar alternation? We can also concatenate bar alternation, but we can't just swap, say, `[~!@]` for `~|!|@`. This is because we need to distinguish between an option in bar notation, which in itself could be a concatenation of symbols, and a new symbol.\n",
    "\n",
    "Let's see an example. Say that we wanted to process a document and gather answers from a survey to \"Yes\" or \"No\" questions. We could use a regular expression to match exactly these options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbdec5e1736ac044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular expression \"Yes|No\" does not match string \"\".\n",
      "Regular expression \"Yes|No\" does not match string \"Y\".\n",
      "Regular expression \"Yes|No\" does not match string \"N\".\n",
      "Regular expression \"Yes|No\" matches \"Yes\".\n",
      "Regular expression \"Yes|No\" matches \"No\".\n",
      "Regular expression \"Yes|No\" does not match string \"Maybe\".\n",
      "Regular expression \"Yes|No\" does not match string \"Don't Know\".\n"
     ]
    }
   ],
   "source": [
    "from utils.widgets.regex_verifier import RegexVerifier\n",
    "\n",
    "concatenation_pattern: str = \"Yes|No\"\n",
    "\n",
    "strings: list[str] = [\n",
    "\t\"\",\n",
    "\t\"Y\",\n",
    "\t\"N\",\n",
    "\t\"Yes\",\n",
    "\t\"No\",\n",
    "\t\"Maybe\",\n",
    "\t\"Don't Know\"\n",
    "]\n",
    "\n",
    "for concatenation_string in strings:\n",
    "\tRegexVerifier.apply_regex(concatenation_pattern, concatenation_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f7ac6a461c41d2",
   "metadata": {},
   "source": [
    "We can see that this regular expression matches only the concatenated options `\"Yes\"` and `\"No\"`--not shortened forms (`\"Y\"`, `\"N\"`). In short, unlike bracket alternation, bar alternation can consist of more complex alternatives. We'll expand on this further in a later section (see Grouping).\n",
    "\n",
    "To summarize, we've now gotten a sense of *concatenation* in regular expressions. Concatenation joins items together in a specified order, and Python's syntax allows for this simply by appending each portion of a pattern to the next.\n",
    "\n",
    "To check your understanding, feel free to try out the exercises below!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ee371e65b55c5c",
   "metadata": {},
   "source": [
    "#### Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c57cb1b2a784d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5d9104fd3a4edabc7cdd93eb0abdb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultipleChoiceQuestion(children=(Output(), RadioButtons(layout=Layout(width='max-content'), options=('[a-zA-Z]…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.activities.question_table import QUESTION_TABLE\n",
    "\n",
    "display(QUESTION_TABLE[\"password-three-two-one\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e41d009253dd4ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268f9b6fc7184274ae7aca57de2027fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultipleChoiceQuestion(children=(Output(), RadioButtons(layout=Layout(width='max-content'), options=('[Strongl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.activities.question_table import QUESTION_TABLE\n",
    "\n",
    "display(QUESTION_TABLE[\"survey-options\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d90f7659cc0e7ea",
   "metadata": {},
   "source": [
    "### Quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc330ffc8268158",
   "metadata": {},
   "source": [
    "So far, we've been able to pick between a variety of options (alternation) and combine them together in a specified order (concatenation). With these mechanisms, we can create passwords of decent complexity. However, even with choice, this manner of password requires a rigid templatic structure. This is disadvantageous in two ways:\n",
    "1. First, it retains security issues. Our current operators do not permit any variation in the number of symbols: each alternation ultimately results in one symbol, and we must specify the number of and order of the alternations that must be satisfied. As a result, if someone were to figure out that a password must be a certain number of characters, they could vastly diminish the search space for passwords.\n",
    "2. Second, it is tedious to write; we must specify the number of permitted characters *exactly*, as we have no way of making some items optional or able to be used repeatedly.\n",
    "\n",
    "Thankfully, both the original regular expressions and Python's implementation of them implement a third operator to solve this problem: *quantification*. In fact, the original formalism and Python use the same symbol to represent quantification: `*`. Both append this symbol after the alternation that they modify, permitting the use of *zero or more* of items in that alternation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c922148b4a57ad",
   "metadata": {},
   "source": [
    "Let's consider some examples of quantification. \n",
    "\n",
    "Before we move back to our password example, let's start off with a simple one. As you may know, all data in a computer is stored as bits, commonly referred to as `0`s and `1`s. We can form the space of all possible bit strings as follows: `[01]*`. We can see the variety of this strings that this accepts below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "126695db59a7baf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular expression \"[01]*\" matches \"\".\n",
      "Regular expression \"[01]*\" matches \"0\".\n",
      "Regular expression \"[01]*\" matches \"1\".\n",
      "Regular expression \"[01]*\" matches \"01\".\n",
      "Regular expression \"[01]*\" matches \"10\".\n",
      "Regular expression \"[01]*\" matches \"1010\".\n",
      "Regular expression \"[01]*\" matches \"1100110\".\n"
     ]
    }
   ],
   "source": [
    "from utils.widgets.regex_verifier import RegexVerifier\n",
    "\n",
    "quantification_pattern: str = r\"[01]*\"\n",
    "\n",
    "strings: list[str] = [\n",
    "\t\"\",\n",
    "\t\"0\",\n",
    "\t\"1\",\n",
    "\t\"01\",\n",
    "\t\"10\",\n",
    "\t\"1010\",\n",
    "\t\"1100110\"\n",
    "]\n",
    "\n",
    "for quantification_string in strings:\n",
    "    RegexVerifier.apply_regex(quantification_pattern, quantification_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56fa60f109c3d9b",
   "metadata": {},
   "source": [
    "As you can see, any combination of `\"0\"` and `\"1\"` is matched by this regular expression. This also includes the empty string, as `\"\"` is the result of combining zero `\"0\"`s and zero `\"1\"`s.\n",
    "\n",
    "While affording quite a bit of flexibility, there's some inconvenience to including the empty string; we often want to match *something* rather than *nothing*. Indeed, it doesn't really make sense to have an empty string of bits. We could resolve this by writing `\"[01][01]*\"`, guaranteeing that there will always be at least one bit. \n",
    "\n",
    "However, common practices of writing regular expressions have led to additional syntax that handles this case for us. Instead of writing `*`, we write `+` to mean *one or more* of the items in an alternation. In other words, we could write `\"[01][01]*\"` more simply as `\"[01]+\"`. When we do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "108029cb89bc7735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular expression \"[01]+\" does not match string \"\".\n",
      "Regular expression \"[01]+\" matches \"0\".\n",
      "Regular expression \"[01]+\" matches \"1\".\n",
      "Regular expression \"[01]+\" matches \"01\".\n",
      "Regular expression \"[01]+\" matches \"10\".\n",
      "Regular expression \"[01]+\" matches \"1010\".\n",
      "Regular expression \"[01]+\" matches \"1100110\".\n"
     ]
    }
   ],
   "source": [
    "from utils.widgets.regex_verifier import RegexVerifier\n",
    "\n",
    "quantification_pattern: str = r\"[01]+\"\n",
    "\n",
    "strings: list[str] = [\n",
    "\t\"\",\n",
    "\t\"0\",\n",
    "\t\"1\",\n",
    "\t\"01\",\n",
    "\t\"10\",\n",
    "\t\"1010\",\n",
    "\t\"1100110\"\n",
    "]\n",
    "\n",
    "for quantification_string in strings:\n",
    "    RegexVerifier.apply_regex(quantification_pattern, quantification_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85055ed51b544745",
   "metadata": {},
   "source": [
    "Now, with the introduction of the `+`, we no longer match the empty string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2c7db621b440d2",
   "metadata": {},
   "source": [
    "Let's turn back to our password scenario. Suppose that we have the following constraints on our password:\n",
    "1. Optionally, the password may begin with a capital letter. There is no strict upper limit on the number of uppercase letters.\n",
    "2. After the (optional) capital letters, the password must contain at least one lowercase letter. There is no strict upper limit on the number of lowercase letters.\n",
    "3. The password must contain between two and four numerical digits.\n",
    "4. The password must contain at most one special character of `\"~\"`, `\"!\"`, or `\"@\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9240b4b9f01c56ee",
   "metadata": {},
   "source": [
    "We'll consider each of these constraints in turn.\n",
    "\n",
    "First, we saw in previous sections that the set of all capital letters is `[A-Z]`. Because the constraint on the number of capital letters can be interpreted as \"zero or more\", we can reflect this constraint as: `[A-Z]*`.\n",
    "\n",
    "Second, in a similar manner, we might recall that the set of all lowercase letters is `[a-z]`. We require at least one lowercase letter in this constraint, but we are otherwise unconstrained. As a result, to reflect this \"one or more\" constraint, we could write: `[a-z]+`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd80aca7f7a0bdcb",
   "metadata": {},
   "source": [
    "Third, we return to our shortcut for the set of digits: `\\d`. In this case, our constraint is more complex; we need *at least* two and *at most* four digits. We've seen already how to fulfill the *at least* condition: we could append as many alternations as we wanted with one another. However, with our current syntax, we have no way of making an alternation optional while limiting the number of times it can be used. In other words, thus far, the upper limit has always been unbounded.\n",
    "\n",
    "In cases like this, Python provides a generic syntax to determine the number of times that an alternation should be selected to form a match. This syntax takes the form: `{m,n}`, where `m` and `n` are the lower bound and upper bound on the number of repetitions, respectively. \n",
    "\n",
    "For our password example, the *minimum* number of times that the repetition can occur (`m`) is 2. The *maximum* number of times that the repetition can occur (`n`) is 4. Therefore, to model this constraint, we could write: `\\d{2,4}`.\n",
    "\n",
    "Before we move on to the final constraint, let's see this one in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6c88688977395d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular expression \"\\d{2,4}\" does not match string \"1\".\n",
      "Regular expression \"\\d{2,4}\" matches \"12\".\n",
      "Regular expression \"\\d{2,4}\" matches \"123\".\n",
      "Regular expression \"\\d{2,4}\" matches \"1234\".\n",
      "Regular expression \"\\d{2,4}\" does not match string \"12345\".\n"
     ]
    }
   ],
   "source": [
    "from utils.widgets.regex_verifier import RegexVerifier\n",
    "\n",
    "quantification_pattern: str = r\"\\d{2,4}\"\n",
    "\n",
    "strings: list[str] = [\n",
    "\t\"1\",\n",
    "\t\"12\",\n",
    "\t\"123\",\n",
    "\t\"1234\",\n",
    "\t\"12345\",\n",
    "]\n",
    "\n",
    "for quantification_string in strings:\n",
    "    RegexVerifier.apply_regex(quantification_pattern, quantification_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9908f0a1b31f0ce",
   "metadata": {},
   "source": [
    "We can see that this syntax meets the desired constraints. Numbers with two, three, or four digits are all allowed, whereas those with less or more are not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95df5bc99f72d8a8",
   "metadata": {},
   "source": [
    "Fourth, we need to have *at most* one of our special character set, `[~!@]`. Our `*` and `+` special characters won't help us here, as they set lower bounds. However, our more generic operator, `{m,n}` can.\n",
    "\n",
    "Since we need *at least* zero and *at most* one special character, we could write: `[~!@]{0,1}`. However, `0` is not really a true bound: we can't have negative characters, so `0` is the default value. As a result, we could also write: `[~!@]{,1}`. In other words, we can omit either `m` or `n` (but not both) to exhibit that the minimum or maximum number of characters is unbounded.\n",
    "\n",
    "As it turns out, there is also a *third* way to write this expression. Because our bounds are `0` and `1`, we can also use the special character `?`. This character is equivalent to `{0,1}` and `{,1}`, expressing that its attached alternation is either present once or absent altogether.\n",
    "\n",
    "We'll now take a look at these special characters in practice. The below shows that each of the patterns accepts and rejects the same strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8af4d3dfe11f5f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular expression \"[~!@]{0,1}\" matches \"\".\n",
      "Regular expression \"[~!@]{0,1}\" matches \"~\".\n",
      "Regular expression \"[~!@]{0,1}\" does not match string \"!!\".\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Regular expression \"[~!@]{,1}\" matches \"\".\n",
      "Regular expression \"[~!@]{,1}\" matches \"~\".\n",
      "Regular expression \"[~!@]{,1}\" does not match string \"!!\".\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Regular expression \"[~!@]?\" matches \"\".\n",
      "Regular expression \"[~!@]?\" matches \"~\".\n",
      "Regular expression \"[~!@]?\" does not match string \"!!\".\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from utils.widgets.regex_verifier import RegexVerifier\n",
    "\n",
    "first_pattern: str = \"[~!@]{0,1}\"\n",
    "second_pattern: str = \"[~!@]{,1}\"\n",
    "third_pattern: str = \"[~!@]?\"\n",
    "patterns: list[str] = [first_pattern, second_pattern, third_pattern]\n",
    "strings: list[str] = [\n",
    "\t\"\",\n",
    "\t\"~\",\n",
    "\t\"!!\",\n",
    "]\n",
    "\n",
    "for quantification_pattern in patterns:\n",
    "\tfor quantification_string in strings:\n",
    "\t\tRegexVerifier.apply_regex(quantification_pattern, quantification_string)\n",
    "\telse:\n",
    "\t\tprint(\"-\" * 100)   # Permits break between use of each pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1a1f7b637fd298",
   "metadata": {},
   "source": [
    "With each component of our password properly constrained, we can now filter out invalid passwords that don't meet our criteria. Let's examine the result of our work in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e3f923dbf6ca7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular expression \"[A-Z]*[a-z]+\\d{2,4}[~!@]?\" matches \"Password123@\".\n",
      "Regular expression \"[A-Z]*[a-z]+\\d{2,4}[~!@]?\" matches \"password123@\".\n",
      "Regular expression \"[A-Z]*[a-z]+\\d{2,4}[~!@]?\" matches \"PASSword123@\".\n",
      "Regular expression \"[A-Z]*[a-z]+\\d{2,4}[~!@]?\" does not match string \"PASSWORD123@\".\n",
      "Regular expression \"[A-Z]*[a-z]+\\d{2,4}[~!@]?\" does not match string \"Password1@\".\n",
      "Regular expression \"[A-Z]*[a-z]+\\d{2,4}[~!@]?\" does not match string \"Password12345@\".\n",
      "Regular expression \"[A-Z]*[a-z]+\\d{2,4}[~!@]?\" does not match string \"Password123~!@\".\n",
      "Regular expression \"[A-Z]*[a-z]+\\d{2,4}[~!@]?\" matches \"Password123\".\n"
     ]
    }
   ],
   "source": [
    "from utils.widgets.regex_verifier import RegexVerifier\n",
    "\n",
    "capital_letter_item: str = \"[A-Z]*\"\n",
    "lowercase_letter_item: str = \"[a-z]+\"\n",
    "numerical_digit_item: str = \"\\d{2,4}\"\n",
    "special_character_item: str = \"[~!@]?\"\n",
    "quantification_pattern: str = capital_letter_item + lowercase_letter_item + numerical_digit_item + special_character_item\n",
    "\n",
    "strings: list[str] = [\n",
    "\t\"Password123@\",\n",
    "\t\"password123@\",\n",
    "\t\"PASSword123@\",\n",
    "\t\"PASSWORD123@\",\n",
    "\t\"Password1@\",\n",
    "\t\"Password12345@\",\n",
    "\t\"Password123~!@\",\n",
    "\t\"Password123\"\n",
    "]\n",
    "\n",
    "for quantification_string in strings:\n",
    "\tRegexVerifier.apply_regex(quantification_pattern, quantification_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a15ef8b51e4881f",
   "metadata": {},
   "source": [
    "Of the eight potential passwords, four meet our requirements and four do not. Although there's a lot we could say about these eight examples in aggregate, we'll summarize the main takeaways here:\n",
    "- Passwords 1, 2, and 3 show that initial capital letters are possible but not required.\n",
    "- Passwords 3 and 4 show that at least one lowercase letter is required; the omission of the lowercase letter in the fourth password causes its rejection.\n",
    "- Passwords 5 and 6 show the effectiveness of our digit constraint, as 1 and 5 digits are not within the instituted 2-4 digit range.\n",
    "- Passwords 7 and 8, when contrasted against the other accepted passwords, exhibit that up to one special character is permitted. Having no special characters is also valid.\n",
    "\n",
    "With that covered, we've now seen the variety of ways in which Python implements quantification in regular expressions. Feel free to check your understanding of quantification with the exercises below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dcccb2c98eace0",
   "metadata": {},
   "source": [
    "#### Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa1d538d9ef93a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acba0290fb694640befc898377ad3dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultipleChoiceQuestion(children=(Output(), RadioButtons(layout=Layout(width='max-content'), options=('-[1-9]\\\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.activities.question_table import QUESTION_TABLE\n",
    "\n",
    "display(QUESTION_TABLE[\"practical-negative-numbers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed84502606dc4a05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T18:51:22.173926Z",
     "start_time": "2024-09-10T18:51:22.166792Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ba34ddab5de44e4b59d86689e7bcae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultipleChoiceQuestion(children=(Output(), RadioButtons(layout=Layout(width='max-content'), options=('[1-9]\\\\d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.activities.question_table import QUESTION_TABLE\n",
    "\n",
    "display(QUESTION_TABLE[\"practical-decimals\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cf29da5af1a3ce",
   "metadata": {},
   "source": [
    "### Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4b5ec59be7e838",
   "metadata": {},
   "source": [
    "So far, we've introduced three powerful features boasted by Python's regular expressions. However, these features still have limitations. For now, we'll turn our attention away from the recurrent password example to examine three cases where these limitations are more pronounced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a865479c5541ff8",
   "metadata": {},
   "source": [
    "#### Problem 1: Complex Quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1447e51890ba377",
   "metadata": {},
   "source": [
    "Previously, you saw that we could model bit strings with the regular expression `[01]+`. However, what if we wanted to model *bytes*? We define a byte as a sequence of eight bits. It cannot be seven or nine bits--it must be exactly eight bits.\n",
    "\n",
    "Let's start by modeling a byte. We've been given a constraint of exactly *eight* repetitions of our alternation. Thus, for this, we'd write `[01]{8,8}`. As the code block below displays, this does exactly what we'd want:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cdfee3cc02a77641",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T18:51:22.576257Z",
     "start_time": "2024-09-10T18:51:22.567981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular expression \"[01]{8,8}\" does not match string \"1111000\".\n",
      "Regular expression \"[01]{8,8}\" matches \"11110000\".\n",
      "Regular expression \"[01]{8,8}\" does not match string \"111100000\".\n"
     ]
    }
   ],
   "source": [
    "from utils.widgets.regex_verifier import RegexVerifier\n",
    "\n",
    "grouping_pattern: str = r\"[01]{8,8}\"\n",
    "strings: list[str] = [\n",
    "\t\"1111000\",\n",
    "\t\"11110000\",\n",
    "\t\"111100000\"\n",
    "]\n",
    "\n",
    "for grouping_string in strings:\n",
    "    RegexVerifier.apply_regex(grouping_pattern, grouping_string, show_groups=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321ea5272698a22f",
   "metadata": {},
   "source": [
    "Problems arise when we consider that we can't just model a single *byte*. Rather, we want to gather entire sequences of bytes. If we assume that these byte sequences must be perfect multiples of 8, our quantification notation becomes insufficient. For instance, `[01]{8,24}` might model sequences of one, two, and three bytes, but it will also include incomplete bytes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff47c169d76773ea",
   "metadata": {},
   "source": [
    "#### Problem 2: Ambiguous Modification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a145d1788cb1f7",
   "metadata": {},
   "source": [
    "This problem with more complex patterns of quantification extends to other scenarios involving concatenation, as well. Suppose that we wanted to collect sentences which contain a series of at least three nouns; in other words, some variation on \"X, Y, and Z\" or \"X, Y, or Z\". For simplicity, let's make the following assumptions: all nouns have no adjectives modifying them; all series do not begin a sentence; all series employ the Oxford comma. \n",
    "\n",
    "To capture this list of items, we know that each non-final item in the series is going to look like ` \\w+,`, where `\\w` is our word-character shortcut. Meanwhile, the final item will be ` or \\w+` or ` and \\w+`. We could then capture these options in a bar alternation: ` \\w+,| or \\w+| and \\w+`.\n",
    "\n",
    "However, from our discussions of alternation, you might recall that we can only select *one* option with any alternation. Ideally, then, we would use quantification to select from our three presented options: for example, we would append a `{3,}` to indicate that we want a series of at least three items. but we are foiled here, too. Our quantification notation may be able to attach to bracket notation, but we currently have no way of saying that we are repeating the *entire* bar alternation as opposed to the last character. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f2f6e97e1f1eba",
   "metadata": {},
   "source": [
    "#### Problem 3: The Tedium of Possibilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6abac039e5960a",
   "metadata": {},
   "source": [
    "A third problem we can't solve has less to do with patterns we can't make and more to do with the writability of such patterns. When possible, regular expressions shouldn't require us to write out more than necessary to represent the strings that we want to match. However, when we get more complex chains of concatenation and alternation, we end up doing exactly that.\n",
    "\n",
    "One of our prior exercises discussed survey questions on a Likert scale with five options. Let's suppose here that we have further options for finer granularity. For example, a review for a service might ask you to choose how you felt about that service and provide the following alternatives: \"Very Dissatisfied\", \"Moderately Dissatisfied\", \"Slightly Dissatisfied\", \"Neutral\", \"Slightly Satisfied\", \"Moderately Satisfied\", \"Very Satisfied\". One valid way of modeling this in a regular expression is as follows: `Very Dissatisfied|Moderately Dissatisfied|Slightly Dissatisfied|Neutral|Slightly Satisfied|Moderately Satisfied|Very Satisfied`. \n",
    "\n",
    "You might notice that there's a lot of redundancy in this expression: `\"Dissatisfied\"` and `\"Satisfied\"` occur in three items, whereas `\"Very\"`, `\"Moderately\"`, and `\"Slightly\"` occur in two. In the most abbreviated form, we might select either `\"Neutral\"` or between one of `\"Very\"`, `\"Moderately\"`, and `\"Slightly\"`, and subsequently between one of `\"Dissatisfied\"` and `\"Satisfied\"`. However, we currently have no way of *nesting* bar alternation to model this; as a result, our only option is to write everything out. You can imagine process getting increasingly tedious as a regular expression accrues more sources of alternation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86197f22fb16d63e",
   "metadata": {},
   "source": [
    "#### The Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e43a86c91549394",
   "metadata": {},
   "source": [
    "Thankfully for us, each of the three problems above has one solution: *groups*. Grouping is a powerful concept in Python's regular expression notation. In effect, it permits us to combine a series of alternations together into a unit and modify that unit altogether.\n",
    "\n",
    "Thus far, we've used square brackets for alternation and curly brackets for quantification. You might've noticed that we skipped over regular parentheses; well, this is where they come in! To denote a group in a regular expression, we wrap that group in parentheses. For instance, `([01])` is a group containing a single alternation. We can fit any number of items inside a group.\n",
    "\n",
    "Let's solve each of the problems above by using groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ade5aec609dd19",
   "metadata": {},
   "source": [
    "We'll start with the first. If we want to capture only strings of bytes, then we need to capture units that are multiples of eight. When we group our byte string of length 8, we then can repeatedly gather that group of eight like so: `([01]{8,8})+`. In other words, we gather at least *one* group of eight bits, and we can do so as many times as we need.\n",
    "\n",
    "Let's see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "524f5fd32b51699b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T18:51:23.140811Z",
     "start_time": "2024-09-10T18:51:23.128269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular expression \"([01]{8,8})+\" does not match string \"1111000\".\n",
      "Regular expression \"([01]{8,8})+\" matches \"11110000\".\n",
      "Regular expression \"([01]{8,8})+\" does not match string \"111100000\".\n",
      "Regular expression \"([01]{8,8})+\" does not match string \"111100001111000\".\n",
      "Regular expression \"([01]{8,8})+\" matches \"1111000011110000\".\n",
      "Regular expression \"([01]{8,8})+\" does not match string \"11110000111100000\".\n"
     ]
    }
   ],
   "source": [
    "from utils.widgets.regex_verifier import RegexVerifier\n",
    "\n",
    "grouping_pattern: str = r\"([01]{8,8})+\"\n",
    "strings: list[str] = [\n",
    "\t\"1111000\",   # seven digits\n",
    "\t\"11110000\",   # eight digits\n",
    "\t\"111100000\",   # nine digits\n",
    "\t\"111100001111000\",   # fifteen digits\n",
    "\t\"1111000011110000\",   # sixteen digits\n",
    "\t\"11110000111100000\"   # seventeen digits\n",
    "]\n",
    "\n",
    "for grouping_string in strings:\n",
    "    RegexVerifier.apply_regex(grouping_pattern, grouping_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb93726dcc90c33d",
   "metadata": {},
   "source": [
    "As we can see, this regular expression does the trick: it matches the strings with eight and sixteen bits, and it does not match any strings with incomplete bytes. Here, groups provide us with a convenient way of applying quantifiers to a *sequence* of alternations rather than individual ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59317a2cd3fbe86a",
   "metadata": {},
   "source": [
    "Given the above solution, you might already see how we'll solve our second problem in finding a series.\n",
    "\n",
    "Let's suppose that we capture any beginning and ending of a sentence with `.*` Then, we turn to our series expression. As stated, we want to capture our items in a series at least three times before we match the sentence. Then, we'll need to apply the quantifier `{3,}` to our series expression: `( \\w+,| or \\w+| and \\w+){3,}`.\n",
    "\n",
    "However, this is actually too permissive! Since we are treating these three items as alternatives, we could select the ` or \\w+` or ` and \\w+` options in a non-final position. Let's move this pair of options outside the alternation to reflect this. We also should the number of alternations accordingly to reflect that one of these must be final. This results in: `( \\w+,){2,}( or \\w+| and \\w+)`.\n",
    "\n",
    "With the power of groups, we can abbreviate this notation further. You might've noticed that our `or` and `and` options only swap their conjunction and nothing else. Since everything else is the same, we can rewrite the second group to reflect the actual alternation taking place: `( \\w+,){2,} (and|or) \\w+`.\n",
    "\n",
    "This final regular expression looks quite a bit more like what you'd see in a sentence. Not only does it do what we'd want, but it's also more readable and writable!\n",
    "\n",
    "Let's illustrate that it's functioning properly in the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a53398d43221fd37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T18:51:23.424811Z",
     "start_time": "2024-09-10T18:51:23.418815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular expression \".+( \\w+,){2,} (and|or) \\w+.*\" does not match string \"My favorite color is red.\".\n",
      "Regular expression \".+( \\w+,){2,} (and|or) \\w+.*\" does not match string \"My favorite colors are red and blue.\".\n",
      "Regular expression \".+( \\w+,){2,} (and|or) \\w+.*\" matches \"My favorite colors are red, purple, and blue.\".\n",
      "Regular expression \".+( \\w+,){2,} (and|or) \\w+.*\" matches \"My favorite color is red, purple, or blue.\".\n",
      "Regular expression \".+( \\w+,){2,} (and|or) \\w+.*\" matches \"My favorite colors are red, orange, purple, and blue.\".\n",
      "Regular expression \".+( \\w+,){2,} (and|or) \\w+.*\" does not match string \"My favorite colors are red and blue, but I don't like green or yellow.\".\n",
      "Regular expression \".+( \\w+,){2,} (and|or) \\w+.*\" matches \"My favorite color is red, purple, or blue, but I don't like green or yellow.\".\n"
     ]
    }
   ],
   "source": [
    "from utils.widgets.regex_verifier import RegexVerifier\n",
    "\n",
    "grouping_pattern: str = r\".+( \\w+,){2,} (and|or) \\w+.*\"\n",
    "strings: list[str] = [\n",
    "\t\"My favorite color is red.\", \n",
    "\t\"My favorite colors are red and blue.\",\n",
    "\t\"My favorite colors are red, purple, and blue.\",\n",
    "\t\"My favorite color is red, purple, or blue.\",\n",
    "\t\"My favorite colors are red, orange, purple, and blue.\",\n",
    "\t\"My favorite colors are red and blue, but I don't like green or yellow.\",\n",
    "\t\"My favorite color is red, purple, or blue, but I don't like green or yellow.\"\n",
    "]\n",
    "\n",
    "for grouping_string in strings:\n",
    "    RegexVerifier.apply_regex(grouping_pattern, grouping_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c82de3ef9b760d",
   "metadata": {},
   "source": [
    "Our regular expression evidently meets our requirements. It only matches those strings with a series of at least three items (*i.e.*, #3, #4, #5, and #7)---including ones with multiple potential series (*i.e.*, #7). With this example, we've seen how groups can permit more complex sequences of alternations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcecfd166f898b",
   "metadata": {},
   "source": [
    "Based on our use of groups in the last example, you might be able to imagine how groups can be used to improve writability. For our seven-item Likert scale, we'll start where we stopped. The regular expression `Very Dissatisfied|Moderately Dissatisfied|Slightly Dissatisfied|Neutral|Slightly Satisfied|Moderately Satisfied|Very Satisfied` initially contains seven alternatives.\n",
    "\n",
    "The strings `\"Satisfied\"` and `\"Dissatisfied\"` are both paired with each of `\"Very\"`, `\"Moderately\"`, and `\"Slightly\"`. Meanwhile, `\"Neutral\"` has no relation to these patterns. In short, if we have an item that is either `\"Satisfied\"` or `\"Dissatisfied\"`, we have to choose both the adjective and adverb modifying it.\n",
    "\n",
    "Since there are two choices, we can represent the `\"Satisfied\"` and `\"Dissatisfied\"` variations with two alternations. For them, we can write: `(Very|Moderately|Slightly) (Satisfied|Dissatisfied)`. To this, we can join the `Neutral` option on its own. However, we have to be careful about our grouping; if we wrote `(Satisfied|Dissatisfied)|Neutral`, we would be making a three-way alternation with extra parentheses. \n",
    "\n",
    "Because `\"Neutral\"` isn't modified at all, we can nest the two alternations in a larger group and alternate `\"Neutral\"` with that to properly match all our options. This would result in the expression `((Very|Moderately|Slightly) (Satisfied|Dissatisfied))|Neutral`.\n",
    "\n",
    "We test this expression in the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2505455a1cddfb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T18:51:23.500591Z",
     "start_time": "2024-09-10T18:51:23.493813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular expression \"((Very|Moderately|Slightly) (Satisfied|Dissatisfied))|Neutral\" matches \"Very Satisfied\".\n",
      "Regular expression \"((Very|Moderately|Slightly) (Satisfied|Dissatisfied))|Neutral\" matches \"Moderately Satisfied\".\n",
      "Regular expression \"((Very|Moderately|Slightly) (Satisfied|Dissatisfied))|Neutral\" matches \"Slightly Satisfied\".\n",
      "Regular expression \"((Very|Moderately|Slightly) (Satisfied|Dissatisfied))|Neutral\" matches \"Neutral\".\n",
      "Regular expression \"((Very|Moderately|Slightly) (Satisfied|Dissatisfied))|Neutral\" does not match string \"Very Neutral\".\n",
      "Regular expression \"((Very|Moderately|Slightly) (Satisfied|Dissatisfied))|Neutral\" matches \"Slightly Dissatisfied\".\n",
      "Regular expression \"((Very|Moderately|Slightly) (Satisfied|Dissatisfied))|Neutral\" matches \"Moderately Dissatisfied\".\n",
      "Regular expression \"((Very|Moderately|Slightly) (Satisfied|Dissatisfied))|Neutral\" matches \"Very Dissatisfied\".\n"
     ]
    }
   ],
   "source": [
    "from utils.widgets.regex_verifier import RegexVerifier\n",
    "\n",
    "grouping_pattern: str = \"((Very|Moderately|Slightly) (Satisfied|Dissatisfied))|Neutral\"\n",
    "strings: list[str] = [\n",
    "\t\"Very Satisfied\",\n",
    "\t\"Moderately Satisfied\",\n",
    "\t\"Slightly Satisfied\",\n",
    "\t\"Neutral\",\n",
    "\t\"Very Neutral\",\n",
    "\t\"Slightly Dissatisfied\",\n",
    "\t\"Moderately Dissatisfied\",\n",
    "\t\"Very Dissatisfied\",\n",
    "]\n",
    "\n",
    "for grouping_string in strings:\n",
    "    RegexVerifier.apply_regex(grouping_pattern, grouping_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad48b407a0d0df50",
   "metadata": {},
   "source": [
    "You'll see here that all of our Likert scale options match. Moreover, due to our careful grouping, the combined \"Very Neutral\" string is not matched, as \"Very\" has no way of combining with \"Neutral\". This use of grouping displays its ability to write regular expressions more succinctly.  \n",
    "\n",
    "As it turns out, this isn't all groups have to offer; in fact, there's a load of additional syntax that permits an extensive use of groups. While we won't address it all in this notebook, we'll touch on a few handy constructions that Python supports."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f15020e4ab969b4",
   "metadata": {},
   "source": [
    "#### Specialized Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c44d605030bb8d",
   "metadata": {},
   "source": [
    "Throughout our examples this far, we've shown how regular expressions are powerful pattern-matching tools. We've seen many uses of these patterns for selecting and filtering data points. In some applications, however, we don't just want to match a pattern; we want specific pieces of information *inside* that pattern.\n",
    "\n",
    "In Python's `re` module, groups aren't just a useful feature in regular expressions. They can also be captured by the module and are accessible through its API. \n",
    "\n",
    "By default, groups are *capturing* groups: the library will keep track of what portion of a string is matched by each group in a regular expression. Python's regular expression syntax allows us to create *non-capturing* groups, as well. If we know that some of the string isn't important to us, we can use groups to discard the information that doesn't matter and extract the information that does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1f8d8c557556a7",
   "metadata": {},
   "source": [
    "Any specialized group in Python is introduced by placing the `?` character after the first parenthesis of the group. We specify the type of group with further characters. As an example, let's briefly examine non-capturing groups. These groups join `?` with a `:` symbol. In other words, a non-capturing group might look like: `(?:<expression>)`, where `<expression>` is replaced with any subexpression of choice. \n",
    "\n",
    "These groups are useful when we might functionally need a group in our regular expression, but we don't actually need that group's matching text to be saved. For instance, in our Likert scale example, if we wanted to boil down our set of seven categories to three, we could adjust our groups to the following expression: `((?:(?:Very|Moderately|Slightly) (?:Satisfied|Dissatisfied))|Neutral)`. Then, instead of checking the match directly for each string, we'd examine Group 1 of the returned object to determine whether the matched categorization was, in general, `\"Satisfied\"`, `\"Dissatisfied\"`, or `\"Neutral\"`. We'll see how to do this in the following section.\n",
    "\n",
    "Another highly useful type of group augments capturing groups to have names. By default, all groups have numbers. The whole match is Group 0; then, each subsequent capturing group inside the regular expression obtains higher group number (*e.g.*, the outermost group for the prior regular expression). However, we can also use Python's regular expression API to access groups by name. Names help to make our regular expressions more readable and also assist in unambiguously accessing the values that we want.\n",
    "\n",
    "It's been a little while, so you might not remember--but we've actually seen named groups before! At the very beginning of this section, we gave a regular expression to identify times on a 24-hour clock: `(?P<hour>[01][0-9]|[2][0-3]):(?P<minute>[0-5][0-9])` .\n",
    "\n",
    "You can see from this expression a new symbol and notation to show names. To indicate that we are using a named group, we follow up the `?` symbol with a `P`. Then, in triangular brackets, we write the name of our group. Names *must* be unique in a regular expression; otherwise, there are few constraints on them.\n",
    "\n",
    "We've talked a bit about numbered and named groups, but let's take a peek at some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1c8c1fdc364bf9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T18:51:23.631952Z",
     "start_time": "2024-09-10T18:51:23.624590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular expression \"(?P<hour>[01][0-9]|[2][0-3]):(?P<minute>[0-5][0-9])\" matches \"00:01\".\n",
      "\tGroup 1: \"00\"\n",
      "\tGroup 2: \"01\"\n",
      "Regular expression \"(?P<hour>[01][0-9]|[2][0-3]):(?P<minute>[0-5][0-9])\" matches \"00:01\".\n",
      "\tGroup \"hour\": \"00\"\n",
      "\tGroup \"minute\": \"01\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Regular expression \"(?P<hour>[01][0-9]|[2][0-3]):(?P<minute>[0-5][0-9])\" matches \"06:30\".\n",
      "\tGroup 1: \"06\"\n",
      "\tGroup 2: \"30\"\n",
      "Regular expression \"(?P<hour>[01][0-9]|[2][0-3]):(?P<minute>[0-5][0-9])\" matches \"06:30\".\n",
      "\tGroup \"hour\": \"06\"\n",
      "\tGroup \"minute\": \"30\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Regular expression \"(?P<hour>[01][0-9]|[2][0-3]):(?P<minute>[0-5][0-9])\" matches \"12:45\".\n",
      "\tGroup 1: \"12\"\n",
      "\tGroup 2: \"45\"\n",
      "Regular expression \"(?P<hour>[01][0-9]|[2][0-3]):(?P<minute>[0-5][0-9])\" matches \"12:45\".\n",
      "\tGroup \"hour\": \"12\"\n",
      "\tGroup \"minute\": \"45\"\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Regular expression \"(?P<hour>[01][0-9]|[2][0-3]):(?P<minute>[0-5][0-9])\" matches \"23:59\".\n",
      "\tGroup 1: \"23\"\n",
      "\tGroup 2: \"59\"\n",
      "Regular expression \"(?P<hour>[01][0-9]|[2][0-3]):(?P<minute>[0-5][0-9])\" matches \"23:59\".\n",
      "\tGroup \"hour\": \"23\"\n",
      "\tGroup \"minute\": \"59\"\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from utils.widgets.regex_verifier import RegexVerifier\n",
    "\n",
    "grouping_pattern: str = \"(?P<hour>[01][0-9]|[2][0-3]):(?P<minute>[0-5][0-9])\"\n",
    "strings: list[str] = [\n",
    "\t\"00:01\",\n",
    "\t\"06:30\",\n",
    "\t\"12:45\",\n",
    "\t\"23:59\"\n",
    "]\n",
    "\n",
    "for grouping_string in strings:\n",
    "\tRegexVerifier.apply_regex(grouping_pattern, grouping_string, show_groups=True)\n",
    "\tRegexVerifier.apply_regex(grouping_pattern, grouping_string, show_named_groups=True)\n",
    "\tprint(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560106c8fb372012",
   "metadata": {},
   "source": [
    "The above displays our two *subgroups* of the expression above. To Python, a regular expression matches one big group; in turn, the groups that we define are portions of that group. That's why our groups are at indices 1 and 2 rather than index 0. To focus on the subgroups, we only display those here---although the entire group is also accessible at index 0.\n",
    "\n",
    "For all four of our valid times, each match is what we'd expect: we get the pair of hours digits in the former subgroup and the pair of minutes digits in the latter subgroup. Indices and names align exactly, permitting us multiple ways of accessing parts of our matched strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66e3799b72a343c",
   "metadata": {},
   "source": [
    "With all that said, we've covered the basics of groups in Python. You've seen the ways in which groups interact with the three basic operations of regular expressions; you've been introduced to capturing and non-capturing groups; and you've seen how named capturing groups can be expressed to enhance readability in writing and applying regular expressions.\n",
    "\n",
    "Before moving on to our final (and short) subsection, feel welcome to check your understanding with the exercises below!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add9afb6aa05cd35",
   "metadata": {},
   "source": [
    "#### Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e1f5a6a0396fb43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T18:51:23.711155Z",
     "start_time": "2024-09-10T18:51:23.691948Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "009deb22927341a58f5ea780a893d53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultipleChoiceQuestion(children=(Output(), RadioButtons(layout=Layout(width='max-content'), options=('[0-9A-F]…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.activities.question_table import QUESTION_TABLE\n",
    "\n",
    "display(QUESTION_TABLE[\"hexadecimals\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9b11e51037d62a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T18:51:23.853529Z",
     "start_time": "2024-09-10T18:51:23.845160Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db5d0c36541a48609f14500e5fb30941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultipleChoiceQuestion(children=(Output(), RadioButtons(layout=Layout(width='max-content'), options=('None; th…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.activities.question_table import QUESTION_TABLE\n",
    "\n",
    "display(QUESTION_TABLE[\"dates\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1bb0a9efb0c35a",
   "metadata": {},
   "source": [
    "### Other Crystals of Syntactic Sugar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe3e5d63fbc592a",
   "metadata": {},
   "source": [
    "At this stage, we've covered four major operations in Python's regular expressions: alternation, concatenation, quantification, and grouping. That's not all there is, though: there are quite a few more bits of notation that you can use to specialize your regular expressions. Some increase the language-recognizing power that regular expressions have even further.\n",
    "\n",
    "Just so you're aware of them, we'll briefly discuss a few below. We'll also specify some of the Python versions where they were added--as some of them were quite recent additions! For more information on these techniques, please refer to Python's documentation.\n",
    "\n",
    "Without further ado, let's take a look at these features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a605af258fbf0c99",
   "metadata": {},
   "source": [
    "**Positional Characters**. Although we use concatenation to indicate where characters are relative to one another, we haven't yet provided a way to determine where the match is relative to the string being matched. Python provides two operators to indicate position: `^` for the beginning of the string and `$` for the end of the string. In other words, these positional operators state that the characters adjacent to them must be at the beginning or end of the string, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfd2ceceed7d388",
   "metadata": {},
   "source": [
    "**Flags**. Groups have a variety of *flags* that can be applied to them. These were added in Python 3.6. In groups, they are placed after the `?` and before a `:`, as they tend to be non-matching groups that define the behavior of the subexpressions they contain. They can restrict matching to ASCII characters or expand it to Unicode characters, ignore letter casing, extend matches across line breaks, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c692e4b52973d32f",
   "metadata": {},
   "source": [
    "**Lookahead and Lookbehind**. Regular non-capturing groups don't impact the overall string matched. However, since Python some group notation can be used to provide constraints on a string being matched that include information at one side or another in a string. For text preceding the string, we use a *lookbehind group*; for text following the string, we use a *lookahead group*. \n",
    "\n",
    "These groups require their contained subexpression to be matched for the whole expression to be matched, but they are not captured in the final result. We can also employ *negative lookahead* and *negative lookbehind* groups; these groups are required *not* to match so that the whole expression can be matched. In other words, we can specify conditions under which our string shouldn't match that might otherwise be false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e39dd89e88b6cd",
   "metadata": {},
   "source": [
    "**Backreferencing Groups**. Named and numbered groups aren't just for use after a string has been matched by a regular expression. A regular expression can also employ *backreferences* to groups, permitting a match to one group to become a pattern for another group. You can reference an indexed group by writing `\\number`, where `number` corresponds to the index of the referenced group. (Note that Group 0 cannot be used.) You can reference a named group by writing `(?P=name)`, where `name` corresponds to the name of the referenced group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3195ff567059b1",
   "metadata": {},
   "source": [
    "**Search Strategies**. When discussing quantification, we described many ways to specify the number of legal repetitions for a match. However, we didn't discuss how quantification decides the number of matches. Python provides three search strategies to govern its approach:\n",
    "- *Greedy Search*: By default, Python will apply quantifiers *greedily*; in other words, it'll match as much as possible before moving onto the next concatenated subexpression. \n",
    "- *Minimal Search*: By adding a `?` after an existing quantifier, we make it *minimal*. In a minimal search, the quantification matches as little text as possible while fulfilling the requirements of the quantifier and regular expression as a whole.\n",
    "- *Possessive Search*: By adding a `+` after an existing quantifier, we make it *possessive*. A possessive quantifier is like a greedy quantifier, but it does not backtrack. Whereas a greedy quantifier can shift part of its match to something else in order to fulfill the requirements of the regular expression as a whole, the possessive quantifier can't: it commits to all its decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81579967770ec4ce",
   "metadata": {},
   "source": [
    "*Congratulations!* You've now gotten an overview of the majority of what Python has to offer in its regular expression notation. \n",
    "\n",
    "In our next section, we'll provide an overview of Python's `re` library to show you how you might use regular expressions to gather, filter, and clean textual data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afb24602b315adc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Python's *re* Library: API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c3ea7222143c39",
   "metadata": {},
   "source": [
    "Python's native regular expression library, known as `re`, operationalizes the notation discussed in the previous section. It provides a variety of functions to *match* and collect relevant data and to *edit* strings in a precise manner. \n",
    "\n",
    "These functionalities center around two classes of objects: `Pattern`s and `Match`es. In the subsections below, we'll examine what the library can do, and we'll also see the utilities that Python gives us to make our lives easier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e48ae0fd807d27",
   "metadata": {},
   "source": [
    "Before we proceed, let's consider a real scenario where we employ these functions. \n",
    "\n",
    "The internet can serve as a vast source of textual data. Many websites are open-access and have already had their contents integrated into large corpora, whether for analytical purposes or for training neural networks. However, just because these websites *can* serve as a source of data doesn't mean that that data is easily accessible or neatly organized. Websites have a variety of purposes, and in turn they are organized through some interface to meet the needs of a designated audience. When someone comes along and wants to scrape data from that website, they usually aren't part of that designated audience: after all, they want to separate the data from the interface precisely designed to present the data. \n",
    "\n",
    "With this premise in mind, we'll mimic two common problems regarding scraping web data in our examples below: determining what to scrape and cleaning the output of the scraping process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39630287e5ff8169",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2910a1213cc2e91f",
   "metadata": {},
   "source": [
    "We begin our exploration of Python's `re` library through its functions. These functions are a user's main point of interaction with the library. To organize our discussion, we'll subdivide these functions into three types:\n",
    "- **Matching** Functions: These functions take at least a regular expression and a candidate string as input, and they output zero or more matches for that string to that regular expression.\n",
    "- **Editing** Functions: These functions take at least a regular expression and a candidate string as input. They output the candidate string with tweaks or divisions made at matching locations.\n",
    "- **Utility** Functions: These functions vary in character. They aid in using the `re` library, providing convenient shortcuts for common operations or promoting efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a50a909bdaf232",
   "metadata": {},
   "source": [
    "#### Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d67c045e5d33e40",
   "metadata": {},
   "source": [
    "Python's `re` library presents a total of five functions to determine whether and how strings are matched by regular expressions. You might think that five is a lot for what would seem to be one operation. However, having these five functions encourages us to think about using regular expressions to solve different problems. \n",
    "\n",
    "These five functions critically differ in three major ways. First, some `re` functions only search from the beginning of the string, whereas others may scan the entire string. Second, some functions check every (non-overlapping) part of the string for a match, whereas others only try to find one match. Third, some functions only produce a match when the entire string is a match, whereas others find and return substrings that match.\n",
    "\n",
    "We'll illustrate how these five functions--`search`, `match`, `fullmatch`, `findall`, and `finditer`--vary in their approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d82b6e7781ceae4",
   "metadata": {},
   "source": [
    "Let's suppose that we're attempting to scrape pages on a dictionary website, and we need to determine which pages to scrape. We'll use Wiktionary for this example, and we'll imagine that our web scraper can gather all the linked headwords from indexing pages like this one: [\"English terms inherited from Old English\"](https://en.wiktionary.org/wiki/Category:English_terms_inherited_from_Old_English). \n",
    "\n",
    "Let's further suppose that we want to collect information only from entries that consist of a single full word that isn't a proper noun. If we examine entries on our linked page, we'll notice that quite a few entries defy this pattern. For example, \"a-\", \"-and\", and \"a- -ing\" are *affixes*: they combine with one or more other words to produce a new word that somehow brings together the meaning of its components. Moreover, \"Abel\", \"Abingdon\", \"Abraham\", and other entries are proper names. Finally, there are a few multiword expressions and idioms: \"alms-fee\", \"apple of someone's eye\", and \"apple tree\", to name a few.\n",
    "\n",
    "Our regular expression needs to filter these out. Thankfully, due to our many restrictions, this isn't too hard. All common nouns are fully lowercase. As a result, we only need to match a sequence of one or more lowercase characters with no spaces. In other words, could use the expression: `[a-z]+`. For readability, let's make this a named group: `(?P<headword>[a-z]+)`.\n",
    "\n",
    "To apply this expression, let's start with the most basic of the matching functions: `match`. The `re` library's `match` function accepts a regular expression and a string as its arguments (along with optional flags). It searches for a match starting from the beginning of the string. It returns a `Match` object if it finds a match; otherwise, it returns `None`. \n",
    "\n",
    "Let's see how `match` does with some of our examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b15fb996737b973",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T18:51:23.921746Z",
     "start_time": "2024-09-10T18:51:23.915533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found for <aback>. Match content: aback\n",
      "Match found for <abb>. Match content: abb\n",
      "Match found for <abbot>. Match content: abbot\n",
      "Match not found for <Abel>.\n",
      "Match not found for <Abingdon>.\n",
      "Match not found for <Abraham>.\n",
      "Match found for <a->. Match content: a\n",
      "Match not found for <-and>.\n",
      "Match found for <a- -ing>. Match content: a\n",
      "Match found for <alms-fee>. Match content: alms\n",
      "Match found for <apple of someone's eye>. Match content: apple\n",
      "Match found for <apple tree>. Match content: apple\n"
     ]
    }
   ],
   "source": [
    "from re import match, Match\n",
    "from typing import Optional\n",
    "\n",
    "headword_pattern: str = \"(?P<headword>[a-z]+)\"\n",
    "potential_headwords: list[str] = [\n",
    "\t\"aback\",   # plain headword\n",
    "\t\"abb\",   # plain headword\n",
    "\t\"abbot\",   # plain headword\n",
    "\t\"Abel\",   # proper noun\n",
    "\t\"Abingdon\",   # proper noun\n",
    "\t\"Abraham\",   # proper noun\n",
    "\t\"a-\",   # affix\n",
    "\t\"-and\",   # affix\n",
    "\t\"a- -ing\",   # affix\n",
    "\t\"alms-fee\",   # multi-word expression\n",
    "\t\"apple of someone's eye\",   # multi-word expression\n",
    "\t\"apple tree\"   # multi-word expression\n",
    "]\n",
    "\n",
    "for headword in potential_headwords:\n",
    "\tpotential_match: Optional[Match] = match(headword_pattern, headword)\n",
    "\tif potential_match is None:   # If we didn't get a match...\n",
    "\t\tprint(f\"Match not found for <{headword}>.\")\n",
    "\telse:   # If we did get a match...\n",
    "\t\tprint(f\"Match found for <{headword}>. Match content: {potential_match['headword']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4cc670c51ba1a5",
   "metadata": {},
   "source": [
    "As we can see, not everything went according to plan. We start off strong; our regular headword entries all matched. Then, we can see that our proper names all are filtered out. However, once we move to our affixes and multi-word expressions, only \"-and\" is correctly handled.\n",
    "\n",
    "What happened? Well, if we look at the content of each match, we'll see there's a certain pattern. The content of matches contains only characters from the *start* of each string, and it proceeds up until there's a character (such as a space or dash) that doesn't fit our pattern.\n",
    "\n",
    "We mentioned that the `match` function searches for a match at the beginning of the input string; however, `match` doesn't require the whole string to match to return a `Match`. In short, the combination of `match` and our regular expression isn't restrictive enough to filter out all our data.\n",
    "\n",
    "We could solve this problem in two ways. We could make our regular expression more strict, or we could use a different function. Since we're focusing on functions here, let's go to the latter.\n",
    "\n",
    "Our solution lies in the `fullmatch` function. This function is exactly like `match`, save that it requires the *whole* input string to match for a `Match` object to be returned. Otherwise, `None` is returned.\n",
    "\n",
    "Let's see it in action in the code block below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4395ae80dfb5c3f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T18:51:24.114030Z",
     "start_time": "2024-09-10T18:51:24.098748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found for <aback>. Match content: aback\n",
      "Match found for <abb>. Match content: abb\n",
      "Match found for <abbot>. Match content: abbot\n",
      "Match not found for <Abel>.\n",
      "Match not found for <Abingdon>.\n",
      "Match not found for <Abraham>.\n",
      "Match not found for <a->.\n",
      "Match not found for <-and>.\n",
      "Match not found for <a- -ing>.\n",
      "Match not found for <alms-fee>.\n",
      "Match not found for <apple of someone's eye>.\n",
      "Match not found for <apple tree>.\n"
     ]
    }
   ],
   "source": [
    "from re import fullmatch, Match\n",
    "from typing import Optional\n",
    "\n",
    "headword_pattern: str = \"(?P<headword>[a-z]+)\"\n",
    "potential_headwords: list[str] = [\n",
    "\t\"aback\",   # plain headword\n",
    "\t\"abb\",   # plain headword\n",
    "\t\"abbot\",   # plain headword\n",
    "\t\"Abel\",   # proper noun\n",
    "\t\"Abingdon\",   # proper noun\n",
    "\t\"Abraham\",   # proper noun\n",
    "\t\"a-\",   # affix\n",
    "\t\"-and\",   # affix\n",
    "\t\"a- -ing\",   # affix\n",
    "\t\"alms-fee\",   # multi-word expression\n",
    "\t\"apple of someone's eye\",   # multi-word expression\n",
    "\t\"apple tree\"   # multi-word expression\n",
    "]\n",
    "\n",
    "for headword in potential_headwords:\n",
    "\tpotential_match: Optional[Match] = fullmatch(headword_pattern, headword)\n",
    "\tif potential_match is None:   # If we didn't get a match...\n",
    "\t\tprint(f\"Match not found for <{headword}>.\")\n",
    "\telse:   # If we did get a match...\n",
    "\t\tprint(f\"Match found for <{headword}>. Match content: {potential_match['headword']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3fe7c390eb59dc",
   "metadata": {},
   "source": [
    "As we can see, `fullmatch` provides us with the desired behavior. Once it finds that the input string stops matching the pattern, it knows to discard it as a match. This fulfills our requirements for selecting dictionary entries.\n",
    "\n",
    "Of course, this isn't to say that `fullmatch` is more useful than `match` by default. Indeed, there are times when `fullmatch` might be too restrictive; for instance, let's suppose that you wanted to capture text in all lines beginning with a list enumeration, such as `\"1. \"`, `\"1.2. \"`, or `\"1.2.3. \"`. You could match this pattern using the following regular expression: `(?P<enumeration>(?:\\d+\\.)+)[\\s]`. Since these lines only *begin* with this content, `fullmatch` would be too restrictive. \n",
    "\n",
    "We hinted above that the dictionary problem could be solved by changing the regular expression instead of the function; as it turns out, that's also possible for our list enumeration problem. In the dictionary problem, we'd need to employ the *positional characters* mentioned in the previous section. You could write: `(?P<headword>^[a-z]+$)` so that the string must contain *only* lowercase alphabetic characters. Meanwhile, in the enumerated list problem, appending `(?P<content>.+)` to the provided regular expression would be enough to permit its use.\n",
    "\n",
    "In short, `match` and `fullmatch` are quite similar as functions; which you should use depends on your way of thinking about solving a problem and, correspondingly, the regular expression that you write."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222e7692b70835d",
   "metadata": {},
   "source": [
    "Not all problems suit the `match` and `fullmatch` functions, however. Let's consider another scenario.\n",
    "\n",
    "Many academic institutions have access to some interface that allows them to search for available papers and journals relevant to their interests. Some of the more creative uses involve tricks that regular expressions use. For instance, you're interested in searching a catalogue of newspapers for the use of the word \"break\" to understand how the term arose and evolved over time, especially regarding the term \"breaking news\".\n",
    "\n",
    "Suppose that your web scraping procedure returns the titles and years of each article in a collection. One way to capture a variety of forms using the word \"break\" would be to use the following regular expression: `(?P<word>[Bb]reak(ing)?|[Bb]roken)`, matching lowercase and titlecase versions of `\"break\"`, `\"breaking\"`, and `\"broken\"`.\n",
    "\n",
    "Our `match` and `fullmatch` functions won't do for this task. Because they only search the beginning of the string, they won't find any internal uses of the word `break`. As a result, we need a different function.\n",
    "\n",
    "We turn to the `search` function. This function attempts to find a match *anywhere* in the input string. If it finds a match, it returns that match--and *only* the first one. Let's try it on a few sample (fictional) titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c1692ac8b30b6f21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T18:51:24.310717Z",
     "start_time": "2024-09-10T18:51:24.304040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match not found for <Update: Criminal Caught after 72-Hour Search>.\n",
      "Match found for <Breaking News: Major Tech Company Announces Unexpected Product>. Match content: Breaking\n",
      "Match found for <What's for Breakfast? Top 10 Dishes>. Match content: Break\n",
      "Match found for <If It Ain't Broken, Don't Fix It>. Match content: Broken\n",
      "Match found for <The break of a century: A man's lottery ticket miracle>. Match content: break\n",
      "Match found for <Breaking the Glass Ceiling: How She Went from Broken to Beyond Her Wildest Dreams>. Match content: Breaking\n"
     ]
    }
   ],
   "source": [
    "from re import search, Match\n",
    "from typing import Optional\n",
    "\n",
    "breaking_pattern: str = \"(?P<word>[Bb]reak(?:ing)?|[Bb]roken)\"\n",
    "headlines: list[str] = [\n",
    "\t\"Update: Criminal Caught after 72-Hour Search\",\n",
    "\t\"Breaking News: Major Tech Company Announces Unexpected Product\",\n",
    "\t\"What's for Breakfast? Top 10 Dishes\",\n",
    "\t\"If It Ain't Broken, Don't Fix It\",\n",
    "\t\"The break of a century: A man's lottery ticket miracle\",\n",
    "\t\"Breaking the Glass Ceiling: How She Went from Broken to Beyond Her Wildest Dreams\"\n",
    "]\n",
    "\n",
    "for headline in headlines:\n",
    "\tpotential_match: Optional[Match] = search(breaking_pattern, headline)\n",
    "\tif potential_match is None:   # If we didn't get a match...\n",
    "\t\tprint(f\"Match not found for <{headline}>.\")\n",
    "\telse:   # If we did get a match...\n",
    "\t\tprint(f\"Match found for <{headline}>. Match content: {potential_match['word']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad183cda05dcfb0",
   "metadata": {},
   "source": [
    "With our regular expression, Python's `search` function was able to find our variations on `\"break\"` in each title. By using the `search` function, we could imagine counting the number of titles per year that use each variation on this word. We could use this information to determine how the frequency of this word in titles changed over time.\n",
    "\n",
    "But what if we want to know how many times the word was used in the title? The `search` function limits itself to only finding *whether* there is a match or not. Therefore, we need a function that doesn't have this limitation.\n",
    "\n",
    "The `findall` and `finditer` functions both don't have that limitation. They are like the `search` function, save for that they seek out *all* non-overlapping matches for a regular expression in a string. The two only differ from each other in terms of what they return; the former returns a list of strings (or tuples when groups are involved), whereas the latter returns an iterator of `Match` objects.\n",
    "\n",
    "For the sake of simplicity, let's apply `findall` to our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b534450e04e9edf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T18:51:24.681314Z",
     "start_time": "2024-09-10T18:51:24.663295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match not found for <Update: Criminal Caught after 72-Hour Search>.\n",
      "Match found for <Breaking News: Major Tech Company Announces Unexpected Product>.\n",
      "\t- Match 1: Breaking\n",
      "Match found for <What's for Breakfast? Top 10 Dishes>.\n",
      "\t- Match 1: Break\n",
      "Match found for <If It Ain't Broken, Don't Fix It>.\n",
      "\t- Match 1: Broken\n",
      "Match found for <The break of a century: A man's lottery ticket miracle>.\n",
      "\t- Match 1: break\n",
      "Match found for <Breaking the Glass Ceiling: How She Went from Broken to Beyond Her Wildest Dreams>.\n",
      "\t- Match 1: Breaking\n",
      "\t- Match 2: Broken\n"
     ]
    }
   ],
   "source": [
    "from re import findall\n",
    "\n",
    "breaking_pattern: str = \"(?P<word>[Bb]reak(?:ing)?|[Bb]roken)\"\n",
    "headlines: list[str] = [\n",
    "\t\"Update: Criminal Caught after 72-Hour Search\",\n",
    "\t\"Breaking News: Major Tech Company Announces Unexpected Product\",\n",
    "\t\"What's for Breakfast? Top 10 Dishes\",\n",
    "\t\"If It Ain't Broken, Don't Fix It\",\n",
    "\t\"The break of a century: A man's lottery ticket miracle\",\n",
    "\t\"Breaking the Glass Ceiling: How She Went from Broken to Beyond Her Wildest Dreams\"\n",
    "]\n",
    "\n",
    "for headline in headlines:\n",
    "\tpotential_matches: list[tuple] = findall(breaking_pattern, headline)\n",
    "\tif len(potential_matches) == 0:   # If we didn't get a match...\n",
    "\t\tprint(f\"Match not found for <{headline}>.\")\n",
    "\telse:   # If we did get a match...\n",
    "\t\tprint(f\"Match found for <{headline}>.\")\n",
    "\t\tfor match_index, match in enumerate(potential_matches, start=1):\n",
    "\t\t\tprint(f\"\\t- Match {match_index}: {match}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf003212c9ba5c6",
   "metadata": {},
   "source": [
    "With the use of `findall`, we can see that the last item now finds both variations on `\"break\"`. By using `findall` or `finditer`, we could provide more precise statistics about the use of the word in titles by incorporating the fact that, at times, the word was used multiple times. If we applied this regular expression to the body of these articles as well, you might imagine this functionality becoming increasingly useful for tabulating word form counts.\n",
    "\n",
    "Using `findall` or `finditer` is exclusively a matter of whether you'd prefer to work with only the matched group outputs or the `Match` objects themselves. The latter provide more information about the matches, so these are the preferable option if more details about the matching are needed. Otherwise, `findall` is sufficient and discards extraneous information for you.\n",
    "\n",
    "Now that we've finished covering those functions, we've seen all five matching functions provided by the `re` library. Next, we'll cover those functions that *edit* text, as opposed to filtering or gathering it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdb8cfd557b9edb",
   "metadata": {},
   "source": [
    "#### Editing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a126389c75c276",
   "metadata": {},
   "source": [
    "It's no small effort to collect text from webpages. But even when that data's been collected, there's no guarantee that it's in a desirable format or free from web artifacts. One strong method for cleaning up scraped text is to use regular expressions.\n",
    "\n",
    "Python gives us three functions to aid us in doing this: `sub`, `subn`, and `split`. The second and third functions are variations on one another. Since there are so few of these functions, instead of categorizing them, we'll proceed right into discussing how they work and what they're useful for."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9e1a83b90048f9",
   "metadata": {},
   "source": [
    "Websites present textual data in a variety of ways using HTML. Not all of these ways are convenient for web scraping, however. For example, one common problem is that a line consists of manually-spaced text. The creator of the website might use manual spacing to spread text across a page in some intended manner. But that text is for *formatting* and not *content*, and frequently we only want the content of that scraping procedure. \n",
    "\n",
    "Python has native functions, such as `strip` or `replace`, to remove spaces at the ends of a string or internally to the string. But, especially when manual spacing is internal to the sentence, these won't solve all our problems: `replace` can be too permissive, as we don't want to remove *all* spaces; rather, we'd want to remove all but one.\n",
    "\n",
    "The `re` library's `sub` function can help us out here. This function allows us to *substitute* text that we match with a regular expression with some replacement string. In other words, variably-matched spans could get replaced by a string we designate.\n",
    "\n",
    "Returning to our whitespace example, we could capture any sequence of spaces with the regular expression: `[ ]+`. However, there are some instances of spaces that we don't want to tweak; for instance, any proper single spaces. As a result, we're looking for any sequence of *two or more* spaces, which we'd write as `[ ]{2,}` using the curly bracket quantification notation. When we match two or more spaces, we'd then want to substitute in a single space.\n",
    "\n",
    "Let's turn to some code to see `sub` in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "adc083cb8e3ec39f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T18:51:24.980014Z",
     "start_time": "2024-09-10T18:51:24.971511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: <The day wasn't over yet,   but I couldn't remember    what else I had to do.>\n",
      "After: <The day wasn't over yet, but I couldn't remember what else I had to do.>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Before: <Section  1:  On  Formal  Logic>\n",
      "After: <Section 1: On Formal Logic>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Before: <There isn't anything wrong with this sentence.>\n",
      "After: <There isn't anything wrong with this sentence.>\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from re import sub\n",
    "\n",
    "substitution_pattern: str = \"[ ]{2,}\"\n",
    "substitution_replacement: str = \" \"\n",
    "spacing_snippets: list[str] = [\n",
    "\t\"The day wasn't over yet,   but I couldn't remember    what else I had to do.\",\n",
    "\t\"Section  1:  On  Formal  Logic\",\n",
    "\t\"There isn't anything wrong with this sentence.\",\n",
    "]\n",
    "\n",
    "for snippet in spacing_snippets:\n",
    "\tsubstituted_snippet: str = sub(substitution_pattern, substitution_replacement, snippet)\n",
    "\tprint(f\"Before: <{snippet}>\\nAfter: <{substituted_snippet}>\")\n",
    "\tprint(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6340d003b364645c",
   "metadata": {},
   "source": [
    "This code block shows what `sub` can do for us. It can edit text under flexible conditions, handling variation in the size and location of matches.\n",
    "\n",
    "As it turns out, `sub` has a few additional mechanisms to increase its adaptability. Initially, we said that it could take a replacement string. However, it can also take a *function* instead. In other words, it can dynamically take each `Match` object that `sub` finds and apply a different kind of replacement to it. When combined with groups, this could be quite potent: you could imagine assigning different cleaning behaviors to different groups and combining many clean-up steps into one function.\n",
    "\n",
    "Another argument that `sub` can take is a count. If you only want to apply a change a maximum number of times, setting this count will enforce that maximum. Let's say that you wanted to filter out strings that required too many edits. Applying this count argument during use of `sub` and then checking whether further matches to your regular expression exist would allow you to see whether the number of edits exceeds the maximum.\n",
    "\n",
    "However, there's another way you could go about this. The `subn` function differs from the `sub` function in that it returns a tuple. The tuple contains the same output as `sub` and then a count of the edits made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50542b7d8c9f4970",
   "metadata": {},
   "source": [
    "The `sub` and `subn` functions are great for changing a string to suit our specifications. However, sometimes we don't want to edit the string so much as we want to divide it into a set of discrete strings.\n",
    "\n",
    "Suppose that we have a text corpus that we'd like to divide up into discrete sentences. How do we determine sentence boundaries? In English, sentences generally end with one of a period, an exclamation point, or a question mark. If we could split textual units based on those punctuation marks, we'd solve our problem.\n",
    "\n",
    "Python does natively provide a `split` function outside the `re` library to do something like this. However, if we were to apply it here, we'd quickly run into a problem: it only permits one delimiter. In other words, even if we used it to split text, we'd have to split text multiple times on each punctuation mark to get all sentence units.\n",
    "\n",
    "Thankfully, the `re` library's `split` function gets around this by using regular expressions. We can match all three punctuation marks with the pattern `[.?!]`. Moreover, to assure that we don't create an empty split at the end of our string, let's assume that all our punctuation is properly spaced out from subsequent sentences. Then, our expression is `[.?!][\\s]`. Let's cut up strings in the code block below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f4ce8c28ac6f762",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T18:51:25.014027Z",
     "start_time": "2024-09-10T18:51:25.008015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Paragraph <1>:\n",
      "\t- Sentence <1>: <I'm a single sentence example.>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Results for Paragraph <2>:\n",
      "\t- Sentence <1>: <This is my first sentence>\n",
      "\t- Sentence <2>: <Is this my second sentence>\n",
      "\t- Sentence <3>: <This is my third sentence>\n",
      "\t- Sentence <4>: <But will there be a fourth sentence?>\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from re import split\n",
    "\n",
    "split_pattern: str = \"[.?!][\\s]\"\n",
    "paragraphs: list[str] = [\n",
    "\t\"I'm a single sentence example.\",\n",
    "\t\"This is my first sentence. \"\n",
    "\t\"Is this my second sentence? \"\n",
    "\t\"This is my third sentence! \"\n",
    "\t\"But will there be a fourth sentence?\",\n",
    "]\n",
    "\n",
    "for paragraph_number, paragraph in enumerate(paragraphs, start=1):\n",
    "\tsentences: list[str] = split(split_pattern, paragraph)\n",
    "\tprint(f\"Results for Paragraph <{paragraph_number}>:\")\n",
    "\tfor sentence_number, sentence in enumerate(sentences, start=1):\n",
    "\t\tprint(f\"\\t- Sentence <{sentence_number}>: <{sentence}>\")\n",
    "\telse:\n",
    "\t\tprint(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7041754d92f2e7e",
   "metadata": {},
   "source": [
    "The `re` library's `split` function did exactly what we wanted here: it returned a list containing each sentence. Including the space after the punctuation also conveniently cut out those spaces from each sentence.\n",
    "\n",
    "As a final note on `split`, we can also add a `maxsplit` argument to determine the number of times that splitting is done. This could be useful if, say, we know beforehand that our data should at most be split into some number of pieces; thus, we could prevent a regular expression from being applied too much for splitting.\n",
    "\n",
    "With that done, we've covered all the editing functions that the `re` library contains. Next, we'll go over the library's utility functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11a2991b2941795",
   "metadata": {},
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b9a2e1ba8702a8",
   "metadata": {},
   "source": [
    "The functions above comprise our major use cases for regular expressions: extracting and tweaking strings. However, there are a few additional tools that `re` provides to work more efficiently. We'll discuss them briefly and provide basic examples for each (as opposed to going through a scenario, since these functions are rather generic)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fca8ebe-856c-4556-8cf5-468fc1141c78",
   "metadata": {},
   "source": [
    "In general, regular expressions are static throughout a program's use: once they've been determined, they aren't changed. However, if we supply the same regular expression string to the functions above, Python theoretically must interpret and validate our expression each time. `re` introduces the `compile` function to avoid that problem. This function takes our regular expression as an argument. It creates a `Pattern` object, allowing our regular expression to be cached and permitting `re`'s functions to apply the pattern right away.\n",
    "\n",
    "We can use the `compile` function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ab4d3e5-0d00-4a91-bd19-5dc780031afe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T18:51:25.143117Z",
     "start_time": "2024-09-10T18:51:25.137015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object Type: <class 're.Pattern'>\n"
     ]
    }
   ],
   "source": [
    "from re import compile, Pattern\n",
    "\n",
    "compiling_expression: str = \"[a-zA-Z]\"\n",
    "\n",
    "my_pattern: Pattern = compile(compiling_expression)\n",
    "print(f\"Object Type: {type(my_pattern)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1344e0-0134-4abe-b45f-3bce83211058",
   "metadata": {},
   "source": [
    "Moving on to our next function, as we've seen above, many punctuation marks and special characters are used in regular expressions. However, sometimes we need to match these characters, too! For instance, in matching any URL, we must escape periods. Otherwise, we'd match many more strings than we intended.\n",
    "\n",
    "Toward this end, Python provides the `escape` function. This function takes our regular expression string as an argument, and it produces that string with any special regular expression characters escaped for us. \n",
    "\n",
    "Let's say that we wanted to match an ellipsis. An ellipsis contains three periods. Unfortunately, a period in regular expression notation is a wildcard character. As a result, while the expression `...` *would* match an ellipsis if we used the expression as-is, it would be incredibly permissive. After all, it would match any string containing three characters! \n",
    "\n",
    "In cases like this, we can use the `escape` function to help us. We present its use below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a208ef4b-fc67-49f0-bb08-a2c0e7525278",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T18:51:25.183781Z",
     "start_time": "2024-09-10T18:51:25.179117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before and After: <...> vs. <\\.\\.\\.>\n"
     ]
    }
   ],
   "source": [
    "from re import escape\n",
    "\n",
    "ellipsis_expression: str = \"...\"\n",
    "escaped_expression: str = escape(ellipsis_expression)\n",
    "print(f\"Before and After: <{ellipsis_expression}> vs. <{escaped_expression}>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d4290f185adb1b",
   "metadata": {},
   "source": [
    "As you can see, the `escape` function will do the work for us of escaping any special characters. We might find this especially useful if we need to insert any longer custom patterns into a regular expression. By using `escape`, we could then insert it into a regular expression string while already having it formatted appropriately. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ebb060-f548-414c-b010-aed664fdca60",
   "metadata": {},
   "source": [
    "Let's move on to our final utility function, `purge`.\n",
    "\n",
    "In many cases, the number of regular expressions in use will be low. When this isn't the case, though, memory may be a concern. Python attempts to cache relevant regular expressions so that it doesn't have to interpret and validate our expressions over and over again. However, as it fills its cache, it must then boot regular expressions from that cache to make space for new ones. This process reduces efficiency by adding extra (and potentially redundant) steps as the program runs.\n",
    "\n",
    "Python provides the `purge` function to give users manual control of the regular expression cache. By calling the `purge` function, which doesn't require any arguments, the regular expression cache is completely cleared.\n",
    "\n",
    "Use of this function isn't terribly common in typical uses of regular expressions, but it may be useful in a resource-intense setting. For example, if a program uses a distinct sets of regular expressions at different stages of a data preprocessing pipeline, it might be safe to purge the cache between those stages so that the cache is fully available for those later stages right away.\n",
    "\n",
    "You could imagine using this in a resource-limited setting, as well. If you had a program running on a device with very limited memory (*e.g.*, IoT devices), even a few regular expressions might fill up vital cache space. Careful use of `purge` would help to navigate the restrictions of this setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ac7ece26330b41",
   "metadata": {},
   "source": [
    "### Objects "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c440617659b7fd1f",
   "metadata": {},
   "source": [
    "Although Python is not solely an object-oriented programming language, it has ample structure to behave like one. Despite largely consisting of functions, Python's `re` library is not an exception to this. The library contains two main classes of objects that programmers can use: `Pattern`s and `Match`es. (In fact, we've already seen both of them in the previous subsection!)\n",
    "\n",
    "Since a good portion of the functionality of these classes is shared with the library as a whole, we'll keep our discussion of these classes brief. Mainly, we'll focus on their distinct features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbe4f7f433c10e3",
   "metadata": {},
   "source": [
    "#### Pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b54e39a1191d92",
   "metadata": {},
   "source": [
    "In Python's `re` library, we can obtain a `Pattern` object by compiling a regular expression. This `Pattern` object caches the expression for its reuse.  \n",
    "\n",
    "The `Pattern` object contains the set of matching and editing functions that the `re` library also contains. Their APIs all differ in one major way: a regular expression does not need to be supplied, as it is already stored in the `Pattern` object.\n",
    "\n",
    "Additionally, for all matching functions, two additional optional arguments may be supplied. These are indices for starting and ending positions of the candidate string. In short, when a regular expression is a `Pattern` object, you're allowed to designate where in the string you want it to work.\n",
    "\n",
    "As for attributes, `Pattern` objects allow for you to retrieve their flags (`flags`), the number of capturing groups they have (`groups`), a dictionary mapping group names to indices (`groupindex`), and the regular expression compiled in the pattern (`pattern`).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c82c46907761552",
   "metadata": {},
   "source": [
    "#### Match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703824d7be932c3f",
   "metadata": {},
   "source": [
    "You might recall seeing the `Match` class appear in many examples above. The `Match` object is a common output to the `re` library's functions. A `Match` designates a span of text matched by a regular expression. It also presents a convenient API to access information about that match. In particular, this API is sensitive to capturing groups which the corresponding regular expression contained.\n",
    "\n",
    "For `Match`, we'll start by looking at the attributes. This class permits you to access:\n",
    "- `re`: the regular expression that produced the `Match` object.\n",
    "- `string`: the string from which the `Match` was produced. \n",
    "- `pos`: the start index for the matching operation.\n",
    "- `endpos`: the end index for the matching operation.\n",
    "- `lastindex`: the last integer index of a matching group, if one exists; if none exist, `None` is returned instead.\n",
    "- `lastgroup`: the name of the last matching group, if one exists; if none exist, `None` is returned instead.\n",
    "\n",
    "Next, this class has a set of functions that indicate information about groups in the match. These compose most of the rest of the additional functionality that a `Match` offers.\n",
    "\n",
    "First, `Match` has a set of functions that indicate the spans of individual groups inside the candidate string. These functions take a group number or name as an argument. The `span` function returns both the starting and ending indices, whereas `start` and `end` return the start index and the end index, respectively.\n",
    "\n",
    "Second, `Match` has another set of functions that return the actual groups matched from the candidate string. If you use dictionary-style key notation (`__getitem__`, used like `dictionary[key]`) on a `Match`, it will return the string corresponding to that group. Both indices and group names work. One or more of these group indicators can be used with the `group` function to return a string (if one indicator is provided) or a tuple (if multiple indicators are provided) containing the string for each group. Meanwhile, the `groups` and `groupdict` functions each return a collection of all subgroups for a `Match`; the former returns an ordered tuple, whereas the latter returns a dictionary and only catalogues named groups.\n",
    "\n",
    "Finally, `Match` has the `expand` function. Since this function uses backreferencing groups, which we only covered briefly in the previous section, we won't go into detail about it here. However, the basic idea of it is that it uses those group references in a template string supplied to it to populate that string. In this way, it's similar to Python's `format` function. For more details, see Python's documentation for this function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96202c22c144e38",
   "metadata": {},
   "source": [
    "Once again, congratulations: you've now gone over the entire `re` library! It's no small feat to have really taken your time with a library and understood how it works, so be sure to give yourself a pat on the back. You've earned it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22402be8adc91a75",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec61160f642b3add",
   "metadata": {},
   "source": [
    "In this notebook, we've walked through the concept of regular expressions.\n",
    "\n",
    "First, you saw the theory of regular expressions. You walked through the three main operations that regular expressions employ: alternation, concatenation, and quantification. You saw examples of their use and visualizations of them through finite automata.\n",
    "\n",
    "Second, you surveyed the practice of regular expressions. You witnessed a plethora of examples describing how to construct regular expressions. These expressions showed you the basic syntax as well as a variety of syntactic sugar for performing alternation, concatenation, quantification, and grouping.\n",
    "\n",
    "Finally, you viewed the API of Python's `re` library. You examined methods for matching and editing text as well as utility functions. You also found out about how to use `re` from an object-oriented perspective with the `Match` and `Pattern` classes.\n",
    "\n",
    "Regular expressions are a powerful mechanism for capturing patterns in strings. On the one hand, they're not a one-stop solution for correcting every issue in text, and they shouldn't be used frivolously. Indeed, Python has a variety of `str` functions that are perfectly acceptable in common text-processing situations. On the other hand, when used carefully, they can provide a precise tool to gather, filter, and edit text in a variety of contexts.\n",
    "\n",
    "While writing regular expressions, whether in theory or in practice, is by no means simple, I hope that this notebook has helped you to make them regular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f7f534c63fb5fa",
   "metadata": {},
   "source": [
    "### Sandbox\n",
    "\n",
    "If you're curious about trying out more regular expressions and strings for matching, there are a variety of public resources available. However, you're also free to use the code produced for this notebook. The interface below will automatically update to show whether and how your string is matched by your regular expression. You can also consult the `regex_verifier.py`'s code for more details on how it was set up as another example for applying the `re` library in practice.\n",
    "\n",
    "Regarding the interface below, note that using named groups overwrites using numbered groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d30e6134deeff2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-10T18:51:25.320303Z",
     "start_time": "2024-09-10T18:51:25.297824Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d8c36c31ef4efd8622d80f774ebc2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(Text(value='', description='Regex:', placeholder='\\\\d*'), Text(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf83187f4004befb570c70c4e333d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.widgets.regex_verifier import RegexVerifier\n",
    "\n",
    "verifier: RegexVerifier = RegexVerifier()\n",
    "verifier.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfa2b0d4fd31d94",
   "metadata": {},
   "source": [
    "Thank you so much for taking your time with this notebook! If you have any questions, comments, concerns, or corrections, please feel free to contact the author of this notebook, Stephen Bothwell (sbothwel@nd.edu). If that email is no longer available, please refer to [his website](https://mythologos.github.io/) for updated contact information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109f590cae4c2113",
   "metadata": {},
   "source": [
    "### Credits\n",
    "\n",
    "The resources below were used in the creation of this notebook. They may also be useful as additional resources for learning about regular expressions in Python, so feel free to consult them, as well!\n",
    "\n",
    "[1] Socratica, Regular Expressions in Python || Python Tutorial || Learn Python Programming, (Dec. 22, 2022). Accessed: Aug. 02, 2024. [Online Video]. Available: https://www.youtube.com/watch?v=nxjwB8up2gI\n",
    "\n",
    "[2] “Python Regular Expressions | Python Education,” Google for Developers. Accessed: Aug. 02, 2024. [Online]. Available: https://developers.google.com/edu/python/regular-expressions\n",
    "\n",
    "[3] A. de Gouvello, aloisdg/awesome-regex. (Aug. 02, 2024). Accessed: Aug. 02, 2024. [Online]. Available: https://github.com/aloisdg/awesome-regex\n",
    "\n",
    "[4] “Python RegEx Archives,” PYnative. Accessed: Aug. 02, 2024. [Online]. Available: https://pynative.com/python/regex/\n",
    "\n",
    "[5] Christopher Woods, “chryswoods.com | Regular Expressions in Python,” chryswoods.com. Accessed: Aug. 02, 2024. [Online]. Available: https://chryswoods.com/intermediate_python/regexp.html\n",
    "\n",
    "[6] Py4DS Community, “18. Regular Expressions, aka regex — Python for Data Science,” Python4DS. Accessed: Aug. 02, 2024. [Online]. Available: https://aeturrell.github.io/python4DS/regex.html#matches-versus-capture-groups"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
